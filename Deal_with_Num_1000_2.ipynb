{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# global variable path\n",
    "MY_PATH = 'E:/pythondata/'\n",
    "# MY_PATH = 'C:/Users/17613/Documents/Calculations/calculations cloud/Python/temp_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxGZiggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n0_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t0ba_X = tba_X\n",
    "t0ba_y = tba_y\n",
    "tv = np.mean(t0ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n8em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t1ba_X = tba_X\n",
    "t1ba_y = tba_y\n",
    "tv = np.mean(t1ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxEpof8PMEPs0nQjcDwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n4em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t2ba_X = tba_X\n",
    "t2ba_y = tba_y\n",
    "tv = np.mean(t2ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvs2JuJ4vCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n2em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t3ba_X = tba_X\n",
    "t3ba_y = tba_y\n",
    "tv = np.mean(t3ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngf4/lISqpqjRWtYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n1em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t4ba_X = tba_X\n",
    "t4ba_y = tba_y\n",
    "tv = np.mean(t4ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC+VPEqkZoeYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n8em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t5ba_X = tba_X\n",
    "t5ba_y = tba_y\n",
    "tv = np.mean(t5ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n7em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t6ba_X = tba_X\n",
    "t6ba_y = tba_y\n",
    "tv = np.mean(t6ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSSAFdxGRBPr/CuXi+aL0waUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n2em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t7ba_X = tba_X\n",
    "t7ba_y = tba_y\n",
    "tv = np.mean(t7ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFcOc4Jv3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_HEADER = 'Num_1000_n1em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t8ba_X = tba_X\n",
    "t8ba_y = tba_y\n",
    "tv = np.mean(t8ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999006910004427\n",
      "0.010547800293430554\n",
      "elapse time = 2.359482526779175\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "X0 = t8ba_X[0:800]\n",
    "y0 = t8ba_y[0:800]\n",
    "y0 = (y0 - np.mean(y0))/np.std(y0)\n",
    "# reg0 = SVR(kernel='rbf', epsilon=1e-2, C=1e10, gamma=1.8e-4)\n",
    "reg0 = SVR(kernel='rbf', epsilon=1e-2, C=1e14, gamma=1e-4)\n",
    "reg0.fit(X0, y0)\n",
    "print(reg0.score(X0,y0))\n",
    "print(np.max(np.abs(reg0.predict(X0) - y0)))\n",
    "t2 = time.time()\n",
    "print('elapse time = ' + str(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999540419767244\n",
      "0.010614395130761789\n",
      "elapse time = 18.271214246749878\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "X0 = t0ba_X[0:3800]\n",
    "y0 = t0ba_y[0:3800]\n",
    "y0 = (y0 - np.mean(y0))/np.std(y0)\n",
    "# reg0 = SVR(kernel='rbf', epsilon=1e-2, C=1e10, gamma=1.8e-4)\n",
    "reg0 = SVR(kernel='rbf', epsilon=1e-2, C=1e14, gamma=1.8e-4)\n",
    "reg0.fit(X0, y0)\n",
    "print(reg0.score(X0,y0))\n",
    "print(np.max(np.abs(reg0.predict(X0) - y0)))\n",
    "t2 = time.time()\n",
    "print('elapse time = ' + str(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999999547206974\n",
      "52625.07015669346\n",
      "0.1971966698546196\n",
      "136692464.81693828\n",
      "0.563645601272583\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "X0 = t8ba_X[0:100]\n",
    "y0 = t8ba_y[0:100]\n",
    "# reg0 = SVR(kernel='rbf', epsilon=1e-2, C=1e10, gamma=1.8e-4)\n",
    "reg0 = SVR(kernel='rbf', epsilon=1e-2, C=1e14, gamma=1e-6)\n",
    "reg0.fit(X0, y0)\n",
    "print(reg0.score(X0,y0))\n",
    "print(np.max(np.abs(reg0.predict(X0) - y0)))\n",
    "\n",
    "# X1 = t7ba_X[0:800]\n",
    "# y1 = t7ba_y[0:800]\n",
    "X1 = t8ba_X[1500:-1]\n",
    "y1 = t8ba_y[1500:-1]\n",
    "print(reg0.score(X1,y1))\n",
    "print(np.max(np.abs(reg0.predict(X1) - y1)))\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-73.37228518356314\n",
      "123163251.38881421\n"
     ]
    }
   ],
   "source": [
    "X1 = t4ba_X[0:3800]\n",
    "y1 = t4ba_y[0:3800]\n",
    "print(reg0.score(X1,y1))\n",
    "print(np.max(np.abs(reg0.predict(X1) - y1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091549.7133574486\n",
      "32821230.332074568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.4850071e-06"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:400], tba_y[0:400], test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "print(np.mean(y_train) - np.mean(y_test))\n",
    "print(np.std(y_train))\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "y_train = (y_train - np.mean(y_train))/np.std(y_train)\n",
    "y_test = (y_test - np.mean(y_train))/np.std(y_train)\n",
    "\n",
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing accuracy 1.5388275  training accuracy:1.4659863\n",
      "lr00 Iter 100 , testing accuracy 1.1211119  training accuracy:0.9778347\n",
      "lr00 Iter 200 , testing accuracy 0.97957206  training accuracy:0.7926104\n",
      "lr00 Iter 300 , testing accuracy 0.8668571  training accuracy:0.639996\n",
      "lr00 Iter 400 , testing accuracy 0.77215844  training accuracy:0.5113524\n",
      "lr00 Iter 500 , testing accuracy 0.6801362  training accuracy:0.40111542\n",
      "lr00 Iter 600 , testing accuracy 0.60527366  training accuracy:0.31906745\n",
      "lr00 Iter 700 , testing accuracy 0.54004073  training accuracy:0.2592777\n",
      "lr00 Iter 800 , testing accuracy 0.49595878  training accuracy:0.22706065\n",
      "lr00 Iter 900 , testing accuracy 0.4428783  training accuracy:0.185762\n",
      "lr00 Iter 1000 , testing accuracy 0.41153672  training accuracy:0.16696166\n",
      "lr00 Iter 1100 , testing accuracy 0.38112956  training accuracy:0.15068495\n",
      "lr00 Iter 1200 , testing accuracy 0.37638065  training accuracy:0.15867688\n",
      "lr00 Iter 1300 , testing accuracy 0.35271335  training accuracy:0.14780432\n",
      "lr00 Iter 1400 , testing accuracy 0.33256203  training accuracy:0.13777399\n",
      "lr00 Iter 1500 , testing accuracy 0.31174582  training accuracy:0.12593096\n",
      "lr00 Iter 1600 , testing accuracy 0.28531647  training accuracy:0.10794771\n",
      "lr00 Iter 1700 , testing accuracy 0.27253568  training accuracy:0.10279224\n",
      "lr00 Iter 1800 , testing accuracy 0.2566395  training accuracy:0.0931019\n",
      "lr00 Iter 1900 , testing accuracy 0.24677189  training accuracy:0.09071353\n",
      "lr00 Iter 2000 , testing accuracy 0.25937977  training accuracy:0.10830929\n",
      "lr00 Iter 2100 , testing accuracy 0.2575065  training accuracy:0.11196657\n",
      "lr00 Iter 2200 , testing accuracy 0.21153226  training accuracy:0.073281184\n",
      "lr00 Iter 2300 , testing accuracy 0.22409545  training accuracy:0.09040923\n",
      "lr00 Iter 2400 , testing accuracy 0.20346266  training accuracy:0.07347684\n",
      "lr00 Iter 2500 , testing accuracy 0.20535672  training accuracy:0.07980849\n",
      "lr00 Iter 2600 , testing accuracy 0.18204382  training accuracy:0.060088396\n",
      "lr00 Iter 2700 , testing accuracy 0.1880315  training accuracy:0.06887599\n",
      "lr00 Iter 2800 , testing accuracy 0.19622768  training accuracy:0.08057195\n",
      "lr00 Iter 2900 , testing accuracy 0.1833477  training accuracy:0.07152965\n",
      "lr00 Iter 3000 , testing accuracy 0.16843782  training accuracy:0.05871009\n",
      "lr00 Iter 3100 , testing accuracy 0.16315335  training accuracy:0.05508291\n",
      "lr00 Iter 3200 , testing accuracy 0.16032085  training accuracy:0.05547978\n",
      "lr00 Iter 3300 , testing accuracy 0.17628828  training accuracy:0.07304297\n",
      "lr00 Iter 3400 , testing accuracy 0.15049423  training accuracy:0.050556798\n",
      "lr00 Iter 3500 , testing accuracy 0.15050077  training accuracy:0.052512486\n",
      "lr00 Iter 3600 , testing accuracy 0.16465029  training accuracy:0.06865825\n",
      "lr00 Iter 3700 , testing accuracy 0.14536633  training accuracy:0.050760858\n",
      "lr00 Iter 3800 , testing accuracy 0.14492396  training accuracy:0.051847484\n",
      "lr00 Iter 3900 , testing accuracy 0.14935787  training accuracy:0.058570787\n",
      "lr00 Iter 4000 , testing accuracy 0.1308325  training accuracy:0.04168042\n",
      "lr00 Iter 4100 , testing accuracy 0.13402398  training accuracy:0.0457021\n",
      "lr00 Iter 4200 , testing accuracy 0.13113602  training accuracy:0.045622226\n",
      "lr00 Iter 4300 , testing accuracy 0.13247126  training accuracy:0.04812224\n",
      "lr00 Iter 4400 , testing accuracy 0.1269201  training accuracy:0.043916896\n",
      "lr00 Iter 4500 , testing accuracy 0.12476522  training accuracy:0.042294573\n",
      "lr00 Iter 4600 , testing accuracy 0.11929937  training accuracy:0.038335796\n",
      "lr00 Iter 4700 , testing accuracy 0.123062  training accuracy:0.043553356\n",
      "lr00 Iter 4800 , testing accuracy 0.114872366  training accuracy:0.035887077\n",
      "lr00 Iter 4900 , testing accuracy 0.11726909  training accuracy:0.039652027\n",
      "lr00 Iter 5000 , testing accuracy 0.12488053  training accuracy:0.048136774\n",
      "lr00 Iter 5100 , testing accuracy 0.11278874  training accuracy:0.03765816\n",
      "lr00 Iter 5200 , testing accuracy 0.11349712  training accuracy:0.038568232\n",
      "lr00 Iter 5300 , testing accuracy 0.10975217  training accuracy:0.03595934\n",
      "lr00 Iter 5400 , testing accuracy 0.11027787  training accuracy:0.037029263\n",
      "lr00 Iter 5500 , testing accuracy 0.110449456  training accuracy:0.03765329\n",
      "lr00 Iter 5600 , testing accuracy 0.10900844  training accuracy:0.03712163\n",
      "lr00 Iter 5700 , testing accuracy 0.10528718  training accuracy:0.03338294\n",
      "lr00 Iter 5800 , testing accuracy 0.10532949  training accuracy:0.034132104\n",
      "lr00 Iter 5900 , testing accuracy 0.10439767  training accuracy:0.03408086\n",
      "lr00 Iter 6000 , testing accuracy 0.10591005  training accuracy:0.036371678\n",
      "lr00 Iter 6100 , testing accuracy 0.10239231  training accuracy:0.033038087\n",
      "lr00 Iter 6200 , testing accuracy 0.10220223  training accuracy:0.033780158\n",
      "lr00 Iter 6300 , testing accuracy 0.10578279  training accuracy:0.038226034\n",
      "lr00 Iter 6400 , testing accuracy 0.100954086  training accuracy:0.033553645\n",
      "lr00 Iter 6500 , testing accuracy 0.10061478  training accuracy:0.033345543\n",
      "lr00 Iter 6600 , testing accuracy 0.10042167  training accuracy:0.033887614\n",
      "lr00 Iter 6700 , testing accuracy 0.09897982  training accuracy:0.032639034\n",
      "lr00 Iter 6800 , testing accuracy 0.09863736  training accuracy:0.032498248\n",
      "lr00 Iter 6900 , testing accuracy 0.098287195  training accuracy:0.032627944\n",
      "lr00 Iter 7000 , testing accuracy 0.10110126  training accuracy:0.036294553\n",
      "lr00 Iter 7100 , testing accuracy 0.09786552  training accuracy:0.032520365\n",
      "lr00 Iter 7200 , testing accuracy 0.09739441  training accuracy:0.032437965\n",
      "lr00 Iter 7300 , testing accuracy 0.09679209  training accuracy:0.03206835\n",
      "lr00 Iter 7400 , testing accuracy 0.09775933  training accuracy:0.03338339\n",
      "lr00 Iter 7500 , testing accuracy 0.09893769  training accuracy:0.035177756\n",
      "lr00 Iter 7600 , testing accuracy 0.09569182  training accuracy:0.03144246\n",
      "lr00 Iter 7700 , testing accuracy 0.095770895  training accuracy:0.03138365\n",
      "lr00 Iter 7800 , testing accuracy 0.095733866  training accuracy:0.03157462\n",
      "lr00 Iter 7900 , testing accuracy 0.098160446  training accuracy:0.03505793\n",
      "lr00 Iter 8000 , testing accuracy 0.095057935  training accuracy:0.031877268\n",
      "lr00 Iter 8100 , testing accuracy 0.09482563  training accuracy:0.031922035\n",
      "lr00 Iter 8200 , testing accuracy 0.097827256  training accuracy:0.035568994\n",
      "lr00 Iter 8300 , testing accuracy 0.09431678  training accuracy:0.031594574\n",
      "lr00 Iter 8400 , testing accuracy 0.09517399  training accuracy:0.0327656\n",
      "lr00 Iter 8500 , testing accuracy 0.09521666  training accuracy:0.033150032\n",
      "lr00 Iter 8600 , testing accuracy 0.09394745  training accuracy:0.03139954\n",
      "lr00 Iter 8700 , testing accuracy 0.09340654  training accuracy:0.031006087\n",
      "lr00 Iter 8800 , testing accuracy 0.09374887  training accuracy:0.031789236\n",
      "lr00 Iter 8900 , testing accuracy 0.094843686  training accuracy:0.03333898\n",
      "lr00 Iter 9000 , testing accuracy 0.094494976  training accuracy:0.033124495\n",
      "lr00 Iter 9100 , testing accuracy 0.09288135  training accuracy:0.03084985\n",
      "lr00 Iter 9200 , testing accuracy 0.09268604  training accuracy:0.030993463\n",
      "lr00 Iter 9300 , testing accuracy 0.09273234  training accuracy:0.030810613\n",
      "lr00 Iter 9400 , testing accuracy 0.093573175  training accuracy:0.031522483\n",
      "lr00 Iter 9500 , testing accuracy 0.093665734  training accuracy:0.03136982\n",
      "lr00 Iter 9600 , testing accuracy 0.09203721  training accuracy:0.0303232\n",
      "lr00 Iter 9700 , testing accuracy 0.09167234  training accuracy:0.029953124\n",
      "lr00 Iter 9800 , testing accuracy 0.09227246  training accuracy:0.031105382\n",
      "lr00 Iter 9900 , testing accuracy 0.09183896  training accuracy:0.030454746\n",
      "lr00 Iter 10000 , testing accuracy 0.091995016  training accuracy:0.030827478\n",
      "lr00 Iter 10100 , testing accuracy 0.09174858  training accuracy:0.030280748\n",
      "lr00 Iter 10200 , testing accuracy 0.09183674  training accuracy:0.030524002\n",
      "lr00 Iter 10300 , testing accuracy 0.09174303  training accuracy:0.030900333\n",
      "lr00 Iter 10400 , testing accuracy 0.091542244  training accuracy:0.030601218\n",
      "lr00 Iter 10500 , testing accuracy 0.09288574  training accuracy:0.032232404\n",
      "lr00 Iter 10600 , testing accuracy 0.09256332  training accuracy:0.03079331\n",
      "lr00 Iter 10700 , testing accuracy 0.09117802  training accuracy:0.030014617\n",
      "lr00 Iter 10800 , testing accuracy 0.09117161  training accuracy:0.030074425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 10900 , testing accuracy 0.09147345  training accuracy:0.029999545\n",
      "lr00 Iter 11000 , testing accuracy 0.09232866  training accuracy:0.03077293\n",
      "lr00 Iter 11100 , testing accuracy 0.09137223  training accuracy:0.030704759\n",
      "lr00 Iter 11200 , testing accuracy 0.09119837  training accuracy:0.02971971\n",
      "lr00 Iter 11300 , testing accuracy 0.093170896  training accuracy:0.03130613\n",
      "lr00 Iter 11400 , testing accuracy 0.09147182  training accuracy:0.030099943\n",
      "lr00 Iter 11500 , testing accuracy 0.09213826  training accuracy:0.030555177\n",
      "lr00 Iter 11600 , testing accuracy 0.09067826  training accuracy:0.029500702\n",
      "lr00 Iter 11700 , testing accuracy 0.09035904  training accuracy:0.02980159\n",
      "lr00 Iter 11800 , testing accuracy 0.09029766  training accuracy:0.029218083\n",
      "lr00 Iter 11900 , testing accuracy 0.090121746  training accuracy:0.029092986\n",
      "lr00 Iter 12000 , testing accuracy 0.090097375  training accuracy:0.029666135\n",
      "lr01 Iter 0 , testing accuracy 0.08977395  training accuracy:0.029019283\n",
      "lr01 Iter 100 , testing accuracy 0.08411202  training accuracy:0.012051699\n",
      "lr01 Iter 200 , testing accuracy 0.0858246  training accuracy:0.01041286\n",
      "lr01 Iter 300 , testing accuracy 0.086023785  training accuracy:0.009966022\n",
      "lr01 Iter 400 , testing accuracy 0.0858323  training accuracy:0.009559775\n",
      "lr01 Iter 500 , testing accuracy 0.08565183  training accuracy:0.009185293\n",
      "lr01 Iter 600 , testing accuracy 0.08547908  training accuracy:0.008843141\n",
      "lr01 Iter 700 , testing accuracy 0.0853129  training accuracy:0.0085305255\n",
      "lr01 Iter 800 , testing accuracy 0.08515149  training accuracy:0.008243975\n",
      "lr01 Iter 900 , testing accuracy 0.08499721  training accuracy:0.007980277\n",
      "lr01 Iter 1000 , testing accuracy 0.08485395  training accuracy:0.0077367052\n",
      "lr01 Iter 1100 , testing accuracy 0.084723875  training accuracy:0.0075108856\n",
      "lr01 Iter 1200 , testing accuracy 0.0846062  training accuracy:0.0073007764\n",
      "lr01 Iter 1300 , testing accuracy 0.08449845  training accuracy:0.0071045747\n",
      "lr01 Iter 1400 , testing accuracy 0.08439948  training accuracy:0.0069207097\n",
      "lr01 Iter 1500 , testing accuracy 0.08430875  training accuracy:0.006747845\n",
      "lr01 Iter 1600 , testing accuracy 0.084225506  training accuracy:0.0065848134\n",
      "lr01 Iter 1700 , testing accuracy 0.084148854  training accuracy:0.0064305835\n",
      "lr01 Iter 1800 , testing accuracy 0.08407802  training accuracy:0.006284254\n",
      "lr01 Iter 1900 , testing accuracy 0.0840122  training accuracy:0.0061450433\n",
      "lr01 Iter 2000 , testing accuracy 0.083950885  training accuracy:0.006012274\n",
      "lr01 Iter 2100 , testing accuracy 0.08389363  training accuracy:0.005885365\n",
      "lr01 Iter 2200 , testing accuracy 0.083840184  training accuracy:0.0057637915\n",
      "lr01 Iter 2300 , testing accuracy 0.08379042  training accuracy:0.0056471163\n",
      "lr01 Iter 2400 , testing accuracy 0.083744116  training accuracy:0.0055349446\n",
      "lr01 Iter 2500 , testing accuracy 0.08370104  training accuracy:0.005426944\n",
      "lr01 Iter 2600 , testing accuracy 0.08366075  training accuracy:0.0053228107\n",
      "lr01 Iter 2700 , testing accuracy 0.08362298  training accuracy:0.005222281\n",
      "lr01 Iter 2800 , testing accuracy 0.083587505  training accuracy:0.0051251166\n",
      "lr01 Iter 2900 , testing accuracy 0.0835541  training accuracy:0.005031094\n",
      "lr01 Iter 3000 , testing accuracy 0.08352252  training accuracy:0.004940026\n",
      "lr01 Iter 3100 , testing accuracy 0.08349258  training accuracy:0.0048517305\n",
      "lr01 Iter 3200 , testing accuracy 0.083464056  training accuracy:0.004766044\n",
      "lr01 Iter 3300 , testing accuracy 0.08343675  training accuracy:0.0046828254\n",
      "lr01 Iter 3400 , testing accuracy 0.08341057  training accuracy:0.004601932\n",
      "lr01 Iter 3500 , testing accuracy 0.08338537  training accuracy:0.004523245\n",
      "lr01 Iter 3600 , testing accuracy 0.08336098  training accuracy:0.00444665\n",
      "lr01 Iter 3700 , testing accuracy 0.08333729  training accuracy:0.0043720426\n",
      "lr01 Iter 3800 , testing accuracy 0.083314165  training accuracy:0.0042993235\n",
      "lr01 Iter 3900 , testing accuracy 0.08329164  training accuracy:0.004228402\n",
      "lr01 Iter 4000 , testing accuracy 0.083269626  training accuracy:0.004159203\n",
      "lr01 Iter 4100 , testing accuracy 0.08324809  training accuracy:0.0040916475\n",
      "lr01 Iter 4200 , testing accuracy 0.083226964  training accuracy:0.0040256637\n",
      "lr01 Iter 4300 , testing accuracy 0.08320631  training accuracy:0.0039611915\n",
      "lr01 Iter 4400 , testing accuracy 0.083186105  training accuracy:0.0038981717\n",
      "lr01 Iter 4500 , testing accuracy 0.08316635  training accuracy:0.0038365496\n",
      "lr01 Iter 4600 , testing accuracy 0.083147034  training accuracy:0.0037762765\n",
      "lr01 Iter 4700 , testing accuracy 0.08312817  training accuracy:0.0037172982\n",
      "lr01 Iter 4800 , testing accuracy 0.083109684  training accuracy:0.003659567\n",
      "lr01 Iter 4900 , testing accuracy 0.08309161  training accuracy:0.0036030426\n",
      "lr01 Iter 5000 , testing accuracy 0.08307392  training accuracy:0.0035476915\n",
      "lr01 Iter 5100 , testing accuracy 0.08305658  training accuracy:0.0034934704\n",
      "lr01 Iter 5200 , testing accuracy 0.08303959  training accuracy:0.0034403424\n",
      "lr01 Iter 5300 , testing accuracy 0.08302293  training accuracy:0.003388274\n",
      "lr01 Iter 5400 , testing accuracy 0.08300662  training accuracy:0.003337232\n",
      "lr01 Iter 5500 , testing accuracy 0.082990624  training accuracy:0.003287188\n",
      "lr01 Iter 5600 , testing accuracy 0.08297495  training accuracy:0.003238112\n",
      "lr01 Iter 5700 , testing accuracy 0.08295954  training accuracy:0.003189974\n",
      "lr01 Iter 5800 , testing accuracy 0.08294444  training accuracy:0.0031427501\n",
      "lr01 Iter 5900 , testing accuracy 0.0829296  training accuracy:0.003096413\n",
      "lr01 Iter 6000 , testing accuracy 0.08291502  training accuracy:0.0030509394\n",
      "lr01 Iter 6100 , testing accuracy 0.08290066  training accuracy:0.0030063037\n",
      "lr01 Iter 6200 , testing accuracy 0.08288653  training accuracy:0.0029624861\n",
      "lr01 Iter 6300 , testing accuracy 0.08287269  training accuracy:0.0029194646\n",
      "lr01 Iter 6400 , testing accuracy 0.082859054  training accuracy:0.0028772175\n",
      "lr01 Iter 6500 , testing accuracy 0.08284563  training accuracy:0.0028357264\n",
      "lr01 Iter 6600 , testing accuracy 0.08283239  training accuracy:0.0027949712\n",
      "lr01 Iter 6700 , testing accuracy 0.08281938  training accuracy:0.0027549348\n",
      "lr01 Iter 6800 , testing accuracy 0.08280659  training accuracy:0.002715601\n",
      "lr01 Iter 6900 , testing accuracy 0.08279398  training accuracy:0.002676947\n",
      "lr01 Iter 7000 , testing accuracy 0.08278155  training accuracy:0.002638962\n",
      "lr01 Iter 7100 , testing accuracy 0.08276927  training accuracy:0.0026016291\n",
      "lr01 Iter 7200 , testing accuracy 0.08275719  training accuracy:0.0025649336\n",
      "lr01 Iter 7300 , testing accuracy 0.08274529  training accuracy:0.0025288588\n",
      "lr01 Iter 7400 , testing accuracy 0.08273355  training accuracy:0.0024933934\n",
      "lr01 Iter 7500 , testing accuracy 0.08272199  training accuracy:0.002458524\n",
      "lr01 Iter 7600 , testing accuracy 0.08271058  training accuracy:0.0024242315\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:3800], tba_y[0:3800], test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "renorm_std = np.std(y_train)\n",
    "renorm_center = np.mean(y_train)\n",
    "\n",
    "y_train = (y_train - renorm_center)/renorm_std\n",
    "y_test = (y_test - renorm_center)/renorm_std\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 800\n",
    "num_layer_3 = 1\n",
    "num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(lr).minimize(loss)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(12001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:0.7, lr:4.0e-2 * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing accuracy \" + str(test_loss) + \"  training accuracy:\" + str(train_loss))\n",
    "        \n",
    "        \n",
    "    for epoch in range(8001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:1.0e-2 * 1.0 ** epoch})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr01 \" + \"Iter \" +str(epoch) + \" , testing accuracy \" + str(test_loss) + \"  training accuracy:\" + str(train_loss))\n",
    "            \n",
    "    for epoch in range(8001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:0.7, lr:1.0e-3 * 1.0 ** epoch})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr02 \" + \"Iter \" +str(epoch) + \" , testing accuracy \" + str(test_loss) + \"  training accuracy:\" + str(train_loss))\n",
    "            \n",
    "    for epoch in range(8001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:5.0e-4 * 1.0 ** epoch})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr03 \" + \"Iter \" +str(epoch) + \" , testing accuracy \" + str(test_loss) + \"  training accuracy:\" + str(train_loss))\n",
    "            \n",
    "    for epoch in range(8001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:1.0e-4 * 1.0 ** epoch})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr04 \" + \"Iter \" +str(epoch) + \" , testing accuracy \" + str(test_loss) + \"  training accuracy:\" + str(train_loss))\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing: 93.60196  training :95.845634 lr=0.04\n",
      "lr00 Iter 100 , testing: 0.4773489  training :0.46458712 lr=0.04\n",
      "lr00 Iter 200 , testing: 0.41103357  training :0.25495857 lr=0.04\n",
      "lr00 Iter 300 , testing: 0.36684597  training :0.061870754 lr=0.04\n",
      "lr00 Iter 400 , testing: 0.3539114  training :0.02039596 lr=0.04\n",
      "lr00 Iter 500 , testing: 0.34356347  training :0.013388227 lr=0.04\n",
      "lr00 Iter 600 , testing: 0.3331129  training :0.011266252 lr=0.04\n",
      "lr00 Iter 700 , testing: 0.32282245  training :0.01017992 lr=0.04\n",
      "lr00 Iter 800 , testing: 0.31292203  training :0.009421456 lr=0.04\n",
      "lr00 Iter 900 , testing: 0.3035217  training :0.008811807 lr=0.04\n",
      "lr00 Iter 1000 , testing: 0.29470837  training :0.008311331 lr=0.04\n",
      "lr00 Iter 1100 , testing: 0.28647378  training :0.007870454 lr=0.04\n",
      "lr00 Iter 1200 , testing: 0.27879134  training :0.007471769 lr=0.04\n",
      "lr00 Iter 1300 , testing: 0.27163696  training :0.0071070986 lr=0.04\n",
      "lr00 Iter 1400 , testing: 0.26497155  training :0.0067677773 lr=0.04\n",
      "lr00 Iter 1500 , testing: 0.25878263  training :0.006454631 lr=0.04\n",
      "lr00 Iter 1600 , testing: 0.25305173  training :0.0061582206 lr=0.04\n",
      "lr00 Iter 1700 , testing: 0.24773176  training :0.005885994 lr=0.04\n",
      "lr00 Iter 1800 , testing: 0.24277149  training :0.005621945 lr=0.04\n",
      "lr00 Iter 1900 , testing: 0.2381679  training :0.0053801844 lr=0.04\n",
      "lr00 Iter 2000 , testing: 0.23388878  training :0.005150572 lr=0.04\n",
      "lr00 Iter 2100 , testing: 0.22990648  training :0.0049306215 lr=0.04\n",
      "lr00 Iter 2200 , testing: 0.22619756  training :0.004719458 lr=0.04\n",
      "lr00 Iter 2300 , testing: 0.222739  training :0.0045156907 lr=0.04\n",
      "lr00 Iter 2400 , testing: 0.21950752  training :0.0043180548 lr=0.04\n",
      "lr00 Iter 2500 , testing: 0.21647972  training :0.0041250703 lr=0.04\n",
      "lr00 Iter 2600 , testing: 0.21364167  training :0.003943033 lr=0.04\n",
      "lr00 Iter 2700 , testing: 0.21098182  training :0.0037763698 lr=0.04\n",
      "lr00 Iter 2800 , testing: 0.20848002  training :0.0036181067 lr=0.04\n",
      "lr00 Iter 2900 , testing: 0.2061224  training :0.0034677682 lr=0.04\n",
      "lr00 Iter 3000 , testing: 0.20389634  training :0.0033247701 lr=0.04\n",
      "lr00 Iter 3100 , testing: 0.20179123  training :0.0031892401 lr=0.04\n",
      "lr00 Iter 3200 , testing: 0.19979702  training :0.0030602582 lr=0.04\n",
      "lr00 Iter 3300 , testing: 0.19790547  training :0.0029378573 lr=0.04\n",
      "lr00 Iter 3400 , testing: 0.19610927  training :0.0028217565 lr=0.04\n",
      "lr00 Iter 3500 , testing: 0.194402  training :0.0027114965 lr=0.04\n",
      "lr00 Iter 3600 , testing: 0.19277707  training :0.0026065225 lr=0.04\n",
      "lr00 Iter 3700 , testing: 0.19122937  training :0.00250667 lr=0.04\n",
      "lr00 Iter 3800 , testing: 0.18975292  training :0.0024112675 lr=0.04\n",
      "lr00 Iter 3900 , testing: 0.18834083  training :0.002319663 lr=0.04\n",
      "lr00 Iter 4000 , testing: 0.19012858  training :0.001487955 lr=0.004\n",
      "lr00 Iter 4100 , testing: 0.18711823  training :4.4383924e-06 lr=0.004\n",
      "lr00 Iter 4200 , testing: 0.18725567  training :2.1739406e-06 lr=0.004\n",
      "lr00 Iter 4300 , testing: 0.18735895  training :1.2055472e-06 lr=0.004\n",
      "lr00 Iter 4400 , testing: 0.18742453  training :7.4751506e-07 lr=0.004\n",
      "lr00 Iter 4500 , testing: 0.18746778  training :5.057295e-07 lr=0.004\n",
      "lr00 Iter 4600 , testing: 0.18749838  training :3.6296672e-07 lr=0.004\n",
      "lr00 Iter 4700 , testing: 0.18752079  training :2.7217328e-07 lr=0.004\n",
      "lr00 Iter 4800 , testing: 0.18753809  training :2.1115231e-07 lr=0.004\n",
      "lr00 Iter 4900 , testing: 0.18755148  training :1.6831106e-07 lr=0.004\n",
      "lr00 Iter 5000 , testing: 0.18756215  training :1.3719306e-07 lr=0.004\n",
      "lr00 Iter 5100 , testing: 0.18757068  training :1.1395779e-07 lr=0.004\n",
      "lr00 Iter 5200 , testing: 0.18757752  training :9.608676e-08 lr=0.004\n",
      "lr00 Iter 5300 , testing: 0.18758334  training :8.2056424e-08 lr=0.004\n",
      "lr00 Iter 5400 , testing: 0.18758816  training :7.084358e-08 lr=0.004\n",
      "lr00 Iter 5500 , testing: 0.18759246  training :6.173051e-08 lr=0.004\n",
      "lr00 Iter 5600 , testing: 0.18759616  training :5.4211338e-08 lr=0.004\n",
      "lr00 Iter 5700 , testing: 0.18759944  training :4.800039e-08 lr=0.004\n",
      "lr00 Iter 5800 , testing: 0.18760224  training :4.276383e-08 lr=0.004\n",
      "lr00 Iter 5900 , testing: 0.18760459  training :3.8356255e-08 lr=0.004\n",
      "lr00 Iter 6000 , testing: 0.18760647  training :3.4589345e-08 lr=0.004\n",
      "lr00 Iter 6100 , testing: 0.18760833  training :3.1348183e-08 lr=0.004\n",
      "lr00 Iter 6200 , testing: 0.1876099  training :2.8551508e-08 lr=0.004\n",
      "lr00 Iter 6300 , testing: 0.18761139  training :2.6119466e-08 lr=0.004\n",
      "lr00 Iter 6400 , testing: 0.18761282  training :2.397264e-08 lr=0.004\n",
      "lr00 Iter 6500 , testing: 0.18761426  training :2.2094566e-08 lr=0.004\n",
      "lr00 Iter 6600 , testing: 0.18761553  training :2.0418042e-08 lr=0.004\n",
      "lr00 Iter 6700 , testing: 0.18761668  training :1.8913802e-08 lr=0.004\n",
      "lr00 Iter 6800 , testing: 0.1876178  training :1.7573432e-08 lr=0.004\n",
      "lr00 Iter 6900 , testing: 0.18761879  training :1.6363328e-08 lr=0.004\n",
      "lr00 Iter 7000 , testing: 0.18761984  training :1.528349e-08 lr=0.004\n",
      "lr00 Iter 7100 , testing: 0.18762073  training :1.42991095e-08 lr=0.004\n",
      "lr00 Iter 7200 , testing: 0.1876215  training :1.3421687e-08 lr=0.004\n",
      "lr00 Iter 7300 , testing: 0.18762226  training :1.2611793e-08 lr=0.004\n",
      "lr00 Iter 7400 , testing: 0.18762296  training :1.1871624e-08 lr=0.004\n",
      "lr00 Iter 7500 , testing: 0.18762366  training :1.1194822e-08 lr=0.004\n",
      "lr00 Iter 7600 , testing: 0.18762423  training :1.0576445e-08 lr=0.004\n",
      "lr00 Iter 7700 , testing: 0.18762472  training :1.001764e-08 lr=0.004\n",
      "lr00 Iter 7800 , testing: 0.18762517  training :9.4938555e-09 lr=0.004\n",
      "lr00 Iter 7900 , testing: 0.18762563  training :9.007135e-09 lr=0.004\n",
      "lr00 Iter 8000 , testing: 0.18762606  training :8.559285e-09 lr=0.00040000002\n",
      "lr00 Iter 8100 , testing: 0.1876261  training :8.563793e-09 lr=0.00040000002\n",
      "lr00 Iter 8200 , testing: 0.18762611  training :8.555585e-09 lr=0.00040000002\n",
      "lr00 Iter 8300 , testing: 0.1876261  training :8.5495895e-09 lr=0.00040000002\n",
      "lr00 Iter 8400 , testing: 0.18762608  training :8.5511065e-09 lr=0.00040000002\n",
      "lr00 Iter 8500 , testing: 0.18762606  training :8.5452925e-09 lr=0.00040000002\n",
      "lr00 Iter 8600 , testing: 0.18762603  training :8.5405505e-09 lr=0.00040000002\n",
      "lr00 Iter 8700 , testing: 0.1876261  training :8.539005e-09 lr=0.00040000002\n",
      "lr00 Iter 8800 , testing: 0.18762608  training :8.524624e-09 lr=0.00040000002\n",
      "lr00 Iter 8900 , testing: 0.1876261  training :8.526171e-09 lr=0.00040000002\n",
      "lr00 Iter 9000 , testing: 0.18762608  training :8.522862e-09 lr=0.00040000002\n",
      "lr00 Iter 9100 , testing: 0.18762611  training :8.50996e-09 lr=0.00040000002\n",
      "lr00 Iter 9200 , testing: 0.18762608  training :8.5073895e-09 lr=0.00040000002\n",
      "lr00 Iter 9300 , testing: 0.18762605  training :8.509618e-09 lr=0.00040000002\n",
      "lr00 Iter 9400 , testing: 0.18762611  training :8.504579e-09 lr=0.00040000002\n",
      "lr00 Iter 9500 , testing: 0.18762608  training :8.505322e-09 lr=0.00040000002\n",
      "lr00 Iter 9600 , testing: 0.18762608  training :8.497424e-09 lr=0.00040000002\n",
      "lr00 Iter 9700 , testing: 0.1876261  training :8.489044e-09 lr=0.00040000002\n",
      "lr00 Iter 9800 , testing: 0.1876261  training :8.481442e-09 lr=0.00040000002\n",
      "lr00 Iter 9900 , testing: 0.1876261  training :8.473652e-09 lr=0.00040000002\n",
      "lr00 Iter 10000 , testing: 0.18762611  training :8.470313e-09 lr=0.00040000002\n",
      "lr00 Iter 10100 , testing: 0.18762602  training :8.468261e-09 lr=0.00040000002\n",
      "lr00 Iter 10200 , testing: 0.18762602  training :8.466463e-09 lr=0.00040000002\n",
      "lr00 Iter 10300 , testing: 0.18762605  training :8.465452e-09 lr=0.00040000002\n",
      "lr00 Iter 10400 , testing: 0.18762608  training :8.4641725e-09 lr=0.00040000002\n",
      "lr00 Iter 10500 , testing: 0.18762611  training :8.4640694e-09 lr=0.00040000002\n",
      "lr00 Iter 10600 , testing: 0.18762606  training :8.458126e-09 lr=0.00040000002\n",
      "lr00 Iter 10700 , testing: 0.18762611  training :8.446773e-09 lr=0.00040000002\n",
      "lr00 Iter 10800 , testing: 0.18762605  training :8.446143e-09 lr=0.00040000002\n",
      "lr00 Iter 10900 , testing: 0.18762606  training :8.432741e-09 lr=0.00040000002\n",
      "lr00 Iter 11000 , testing: 0.18762606  training :8.433286e-09 lr=0.00040000002\n",
      "lr00 Iter 11100 , testing: 0.1876261  training :8.427438e-09 lr=0.00040000002\n",
      "lr00 Iter 11200 , testing: 0.18762608  training :8.422999e-09 lr=0.00040000002\n",
      "lr00 Iter 11300 , testing: 0.18762608  training :8.423197e-09 lr=0.00040000002\n",
      "lr00 Iter 11400 , testing: 0.18762608  training :8.419521e-09 lr=0.00040000002\n",
      "lr00 Iter 11500 , testing: 0.18762605  training :8.4082155e-09 lr=0.00040000002\n",
      "lr00 Iter 11600 , testing: 0.1876261  training :8.4090015e-09 lr=0.00040000002\n",
      "lr00 Iter 11700 , testing: 0.18762608  training :8.403575e-09 lr=0.00040000002\n",
      "lr00 Iter 11800 , testing: 0.18762608  training :8.404131e-09 lr=0.00040000002\n",
      "lr00 Iter 11900 , testing: 0.18762612  training :8.39473e-09 lr=0.00040000002\n",
      "lr00 Iter 12000 , testing: 0.1876261  training :8.397964e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12100 , testing: 0.1876261  training :8.396349e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12200 , testing: 0.18762611  training :8.397363e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12300 , testing: 0.18762611  training :8.397e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12400 , testing: 0.18762611  training :8.395448e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12500 , testing: 0.18762614  training :8.3955225e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12600 , testing: 0.18762612  training :8.395801e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12700 , testing: 0.18762611  training :8.396697e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12800 , testing: 0.18762611  training :8.393539e-09 lr=4.0000003e-05\n",
      "lr00 Iter 12900 , testing: 0.18762611  training :8.393495e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13000 , testing: 0.18762612  training :8.3907725e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13100 , testing: 0.18762611  training :8.389422e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13200 , testing: 0.18762611  training :8.3864755e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13300 , testing: 0.18762611  training :8.385317e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13400 , testing: 0.18762611  training :8.386494e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13500 , testing: 0.18762608  training :8.3860465e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13600 , testing: 0.18762611  training :8.388523e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13700 , testing: 0.18762611  training :8.3884135e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13800 , testing: 0.18762608  training :8.389716e-09 lr=4.0000003e-05\n",
      "lr00 Iter 13900 , testing: 0.18762608  training :8.388493e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14000 , testing: 0.18762611  training :8.389279e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14100 , testing: 0.18762611  training :8.388004e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14200 , testing: 0.18762611  training :8.3873815e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14300 , testing: 0.18762611  training :8.387677e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14400 , testing: 0.18762611  training :8.3873175e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14500 , testing: 0.1876261  training :8.3862925e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14600 , testing: 0.18762611  training :8.387452e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14700 , testing: 0.18762611  training :8.3884695e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14800 , testing: 0.18762611  training :8.388028e-09 lr=4.0000003e-05\n",
      "lr00 Iter 14900 , testing: 0.18762611  training :8.386645e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15000 , testing: 0.18762611  training :8.387813e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15100 , testing: 0.18762611  training :8.386257e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15200 , testing: 0.18762611  training :8.385811e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15300 , testing: 0.18762611  training :8.385083e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15400 , testing: 0.18762611  training :8.388132e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15500 , testing: 0.18762612  training :8.389506e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15600 , testing: 0.1876261  training :8.388237e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15700 , testing: 0.1876261  training :8.386316e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15800 , testing: 0.1876261  training :8.386213e-09 lr=4.0000003e-05\n",
      "lr00 Iter 15900 , testing: 0.18762611  training :8.385121e-09 lr=4.0000003e-05\n",
      "lr00 Iter 16000 , testing: 0.18762606  training :8.387025e-09 lr=4e-06\n",
      "lr00 Iter 16100 , testing: 0.18762606  training :8.387025e-09 lr=4e-06\n",
      "lr00 Iter 16200 , testing: 0.18762606  training :8.387213e-09 lr=4e-06\n",
      "lr00 Iter 16300 , testing: 0.18762606  training :8.387213e-09 lr=4e-06\n",
      "lr00 Iter 16400 , testing: 0.18762606  training :8.387213e-09 lr=4e-06\n",
      "lr00 Iter 16500 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 16600 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 16700 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 16800 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 16900 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17000 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17100 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17200 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17300 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17400 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17500 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17600 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17700 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17800 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 17900 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 18000 , testing: 0.18762606  training :8.3871905e-09 lr=4e-06\n",
      "lr00 Iter 18100 , testing: 0.18762608  training :8.38739e-09 lr=4e-06\n",
      "lr00 Iter 18200 , testing: 0.18762608  training :8.38739e-09 lr=4e-06\n",
      "lr00 Iter 18300 , testing: 0.18762608  training :8.38739e-09 lr=4e-06\n",
      "lr00 Iter 18400 , testing: 0.18762608  training :8.38739e-09 lr=4e-06\n",
      "lr00 Iter 18500 , testing: 0.18762608  training :8.38739e-09 lr=4e-06\n",
      "lr00 Iter 18600 , testing: 0.18762608  training :8.38739e-09 lr=4e-06\n",
      "lr00 Iter 18700 , testing: 0.18762608  training :8.387472e-09 lr=4e-06\n",
      "lr00 Iter 18800 , testing: 0.18762608  training :8.387472e-09 lr=4e-06\n",
      "lr00 Iter 18900 , testing: 0.18762608  training :8.387472e-09 lr=4e-06\n",
      "lr00 Iter 19000 , testing: 0.18762608  training :8.388108e-09 lr=4e-06\n",
      "lr00 Iter 19100 , testing: 0.18762608  training :8.388108e-09 lr=4e-06\n",
      "lr00 Iter 19200 , testing: 0.18762608  training :8.3880565e-09 lr=4e-06\n",
      "lr00 Iter 19300 , testing: 0.18762608  training :8.3880565e-09 lr=4e-06\n",
      "lr00 Iter 19400 , testing: 0.18762608  training :8.3880565e-09 lr=4e-06\n",
      "lr00 Iter 19500 , testing: 0.18762608  training :8.3880565e-09 lr=4e-06\n",
      "lr00 Iter 19600 , testing: 0.18762608  training :8.388322e-09 lr=4e-06\n",
      "lr00 Iter 19700 , testing: 0.18762608  training :8.388322e-09 lr=4e-06\n",
      "lr00 Iter 19800 , testing: 0.18762608  training :8.388322e-09 lr=4e-06\n",
      "lr00 Iter 19900 , testing: 0.18762608  training :8.388322e-09 lr=4e-06\n",
      "lr00 Iter 20000 , testing: 0.18762608  training :8.388322e-09 lr=4e-07\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:400], tba_y[0:400]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 800\n",
    "num_layer_3 = 1\n",
    "num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4.0e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=4000,decay_rate=0.1,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing: 40.558773  training :40.770683 lr=0.04\n",
      "lr00 Iter 100 , testing: 0.4106402  training :0.2792077 lr=0.04\n",
      "lr00 Iter 200 , testing: 0.34324142  training :0.1534383 lr=0.04\n",
      "lr00 Iter 300 , testing: 0.29050043  training :0.03134693 lr=0.04\n",
      "lr00 Iter 400 , testing: 0.27824742  training :0.015744358 lr=0.04\n",
      "lr00 Iter 500 , testing: 0.27297163  training :0.01100862 lr=0.04\n",
      "lr00 Iter 600 , testing: 0.26728904  training :0.009360095 lr=0.04\n",
      "lr00 Iter 700 , testing: 0.26144403  training :0.00854624 lr=0.04\n",
      "lr00 Iter 800 , testing: 0.25565526  training :0.008010103 lr=0.04\n",
      "lr00 Iter 900 , testing: 0.2500549  training :0.0075877123 lr=0.04\n",
      "lr00 Iter 1000 , testing: 0.24469405  training :0.0072283233 lr=0.04\n",
      "lr00 Iter 1100 , testing: 0.23957828  training :0.006902044 lr=0.04\n",
      "lr00 Iter 1200 , testing: 0.23469682  training :0.006593406 lr=0.04\n",
      "lr00 Iter 1300 , testing: 0.2300318  training :0.006301777 lr=0.04\n",
      "lr00 Iter 1400 , testing: 0.22556041  training :0.0060187858 lr=0.04\n",
      "lr00 Iter 1500 , testing: 0.22125888  training :0.0057426384 lr=0.04\n",
      "lr00 Iter 1600 , testing: 0.21713518  training :0.0054813353 lr=0.04\n",
      "lr00 Iter 1700 , testing: 0.21319094  training :0.0052307346 lr=0.04\n",
      "lr00 Iter 1800 , testing: 0.20941655  training :0.0049870433 lr=0.04\n",
      "lr00 Iter 1900 , testing: 0.20582928  training :0.004770704 lr=0.04\n",
      "lr00 Iter 2000 , testing: 0.20240675  training :0.0045679365 lr=0.04\n",
      "lr00 Iter 2100 , testing: 0.1991393  training :0.0043739057 lr=0.04\n",
      "lr00 Iter 2200 , testing: 0.19602731  training :0.0041979807 lr=0.04\n",
      "lr00 Iter 2300 , testing: 0.19307418  training :0.004040485 lr=0.04\n",
      "lr00 Iter 2400 , testing: 0.19027415  training :0.0038924818 lr=0.04\n",
      "lr00 Iter 2500 , testing: 0.18762286  training :0.003753712 lr=0.04\n",
      "lr00 Iter 2600 , testing: 0.18511337  training :0.003623127 lr=0.04\n",
      "lr00 Iter 2700 , testing: 0.18273883  training :0.003500444 lr=0.04\n",
      "lr00 Iter 2800 , testing: 0.18049134  training :0.0033846945 lr=0.04\n",
      "lr00 Iter 2900 , testing: 0.17836341  training :0.003275428 lr=0.04\n",
      "lr00 Iter 3000 , testing: 0.17634712  training :0.0031717017 lr=0.04\n",
      "lr00 Iter 3100 , testing: 0.17443514  training :0.0030729384 lr=0.04\n",
      "lr00 Iter 3200 , testing: 0.1726209  training :0.0029785507 lr=0.04\n",
      "lr00 Iter 3300 , testing: 0.1708978  training :0.002887874 lr=0.04\n",
      "lr00 Iter 3400 , testing: 0.16926059  training :0.002800833 lr=0.04\n",
      "lr00 Iter 3500 , testing: 0.16770416  training :0.0027169366 lr=0.04\n",
      "lr00 Iter 3600 , testing: 0.16622409  training :0.0026358562 lr=0.04\n",
      "lr00 Iter 3700 , testing: 0.16481641  training :0.0025572453 lr=0.04\n",
      "lr00 Iter 3800 , testing: 0.16347851  training :0.0024809353 lr=0.04\n",
      "lr00 Iter 3900 , testing: 0.1622099  training :0.0024067222 lr=0.04\n",
      "lr00 Iter 4000 , testing: 0.16071124  training :0.0008027083 lr=0.008\n",
      "lr00 Iter 4100 , testing: 0.15962555  training :2.456473e-06 lr=0.008\n",
      "lr00 Iter 4200 , testing: 0.15972914  training :9.1891314e-07 lr=0.008\n",
      "lr00 Iter 4300 , testing: 0.15981226  training :5.786863e-07 lr=0.008\n",
      "lr00 Iter 4400 , testing: 0.15985568  training :9.16895e-07 lr=0.008\n",
      "lr00 Iter 4500 , testing: 0.15987954  training :1.2870354e-06 lr=0.008\n",
      "lr00 Iter 4600 , testing: 0.15989476  training :1.6918857e-06 lr=0.008\n",
      "lr00 Iter 4700 , testing: 0.15990534  training :2.1117028e-06 lr=0.008\n",
      "lr00 Iter 4800 , testing: 0.15991323  training :2.5341576e-06 lr=0.008\n",
      "lr00 Iter 4900 , testing: 0.15991925  training :2.9623284e-06 lr=0.008\n",
      "lr00 Iter 5000 , testing: 0.15992402  training :3.3964498e-06 lr=0.008\n",
      "lr00 Iter 5100 , testing: 0.1599277  training :3.84834e-06 lr=0.008\n",
      "lr00 Iter 5200 , testing: 0.15993074  training :4.2816546e-06 lr=0.008\n",
      "lr00 Iter 5300 , testing: 0.15993316  training :4.71917e-06 lr=0.008\n",
      "lr00 Iter 5400 , testing: 0.1599352  training :5.154054e-06 lr=0.008\n",
      "lr00 Iter 5500 , testing: 0.15993692  training :5.5816845e-06 lr=0.008\n",
      "lr00 Iter 5600 , testing: 0.15993838  training :6.014748e-06 lr=0.008\n",
      "lr00 Iter 5700 , testing: 0.15993966  training :6.451719e-06 lr=0.008\n",
      "lr00 Iter 5800 , testing: 0.1599407  training :6.88854e-06 lr=0.008\n",
      "lr00 Iter 5900 , testing: 0.15994163  training :7.3039296e-06 lr=0.008\n",
      "lr00 Iter 6000 , testing: 0.15994246  training :7.730506e-06 lr=0.008\n",
      "lr00 Iter 6100 , testing: 0.15994324  training :8.167171e-06 lr=0.008\n",
      "lr00 Iter 6200 , testing: 0.15994379  training :8.607201e-06 lr=0.008\n",
      "lr00 Iter 6300 , testing: 0.15994427  training :9.024752e-06 lr=0.008\n",
      "lr00 Iter 6400 , testing: 0.15994464  training :9.437048e-06 lr=0.008\n",
      "lr00 Iter 6500 , testing: 0.15994515  training :9.885975e-06 lr=0.008\n",
      "lr00 Iter 6600 , testing: 0.15994541  training :1.0314295e-05 lr=0.008\n",
      "lr00 Iter 6700 , testing: 0.15994568  training :1.07359465e-05 lr=0.008\n",
      "lr00 Iter 6800 , testing: 0.15994589  training :1.1165612e-05 lr=0.008\n",
      "lr00 Iter 6900 , testing: 0.15994604  training :1.1576269e-05 lr=0.008\n",
      "lr00 Iter 7000 , testing: 0.15994622  training :1.2022144e-05 lr=0.008\n",
      "lr00 Iter 7100 , testing: 0.15994622  training :1.24220305e-05 lr=0.008\n",
      "lr00 Iter 7200 , testing: 0.15994623  training :1.2858584e-05 lr=0.008\n",
      "lr00 Iter 7300 , testing: 0.15994625  training :1.3302571e-05 lr=0.008\n",
      "lr00 Iter 7400 , testing: 0.15994619  training :1.3684716e-05 lr=0.008\n",
      "lr00 Iter 7500 , testing: 0.15994614  training :1.413592e-05 lr=0.008\n",
      "lr00 Iter 7600 , testing: 0.15994594  training :1.4536747e-05 lr=0.008\n",
      "lr00 Iter 7700 , testing: 0.1599458  training :1.4976871e-05 lr=0.008\n",
      "lr00 Iter 7800 , testing: 0.15994553  training :1.5391703e-05 lr=0.008\n",
      "lr00 Iter 7900 , testing: 0.15994531  training :1.58044e-05 lr=0.008\n",
      "lr00 Iter 8000 , testing: 0.15987924  training :5.840036e-06 lr=0.0016000001\n",
      "lr00 Iter 8100 , testing: 0.15989473  training :7.70813e-11 lr=0.0016000001\n",
      "lr00 Iter 8200 , testing: 0.1598947  training :7.29902e-11 lr=0.0016000001\n",
      "lr00 Iter 8300 , testing: 0.15989472  training :6.991984e-11 lr=0.0016000001\n",
      "lr00 Iter 8400 , testing: 0.1598947  training :6.851581e-11 lr=0.0016000001\n",
      "lr00 Iter 8500 , testing: 0.1598947  training :6.786536e-11 lr=0.0016000001\n",
      "lr00 Iter 8600 , testing: 0.15989472  training :6.658395e-11 lr=0.0016000001\n",
      "lr00 Iter 8700 , testing: 0.15989475  training :6.4998014e-11 lr=0.0016000001\n",
      "lr00 Iter 8800 , testing: 0.15989476  training :6.417135e-11 lr=0.0016000001\n",
      "lr00 Iter 8900 , testing: 0.15989478  training :6.377507e-11 lr=0.0016000001\n",
      "lr00 Iter 9000 , testing: 0.15989478  training :6.286314e-11 lr=0.0016000001\n",
      "lr00 Iter 9100 , testing: 0.15989473  training :6.2280084e-11 lr=0.0016000001\n",
      "lr00 Iter 9200 , testing: 0.15989475  training :6.243194e-11 lr=0.0016000001\n",
      "lr00 Iter 9300 , testing: 0.15989478  training :6.1253655e-11 lr=0.0016000001\n",
      "lr00 Iter 9400 , testing: 0.15989478  training :6.1027905e-11 lr=0.0016000001\n",
      "lr00 Iter 9500 , testing: 0.15989475  training :6.030437e-11 lr=0.0016000001\n",
      "lr00 Iter 9600 , testing: 0.15989475  training :6.055204e-11 lr=0.0016000001\n",
      "lr00 Iter 9700 , testing: 0.15989478  training :5.937214e-11 lr=0.0016000001\n",
      "lr00 Iter 9800 , testing: 0.1598948  training :5.934534e-11 lr=0.0016000001\n",
      "lr00 Iter 9900 , testing: 0.15989475  training :5.865429e-11 lr=0.0016000001\n",
      "lr00 Iter 10000 , testing: 0.15989475  training :5.8007085e-11 lr=0.0016000001\n",
      "lr00 Iter 10100 , testing: 0.15989475  training :5.7202344e-11 lr=0.0016000001\n",
      "lr00 Iter 10200 , testing: 0.15989475  training :5.732334e-11 lr=0.0016000001\n",
      "lr00 Iter 10300 , testing: 0.15989478  training :5.6983904e-11 lr=0.0016000001\n",
      "lr00 Iter 10400 , testing: 0.15989481  training :5.6773582e-11 lr=0.0016000001\n",
      "lr00 Iter 10500 , testing: 0.1598948  training :5.6274173e-11 lr=0.0016000001\n",
      "lr00 Iter 10600 , testing: 0.15989481  training :5.571061e-11 lr=0.0016000001\n",
      "lr00 Iter 10700 , testing: 0.15989476  training :5.4736153e-11 lr=0.0016000001\n",
      "lr00 Iter 10800 , testing: 0.15989478  training :5.4863645e-11 lr=0.0016000001\n",
      "lr00 Iter 10900 , testing: 0.15989484  training :5.4141733e-11 lr=0.0016000001\n",
      "lr00 Iter 11000 , testing: 0.15989481  training :5.419289e-11 lr=0.0016000001\n",
      "lr00 Iter 11100 , testing: 0.15989485  training :5.4244862e-11 lr=0.0016000001\n",
      "lr00 Iter 11200 , testing: 0.15989484  training :5.4270848e-11 lr=0.0016000001\n",
      "lr00 Iter 11300 , testing: 0.15989484  training :5.3548124e-11 lr=0.0016000001\n",
      "lr00 Iter 11400 , testing: 0.15989481  training :5.3202193e-11 lr=0.0016000001\n",
      "lr00 Iter 11500 , testing: 0.15989481  training :5.2946397e-11 lr=0.0016000001\n",
      "lr00 Iter 11600 , testing: 0.15989481  training :5.2609396e-11 lr=0.0016000001\n",
      "lr00 Iter 11700 , testing: 0.15989485  training :5.2307315e-11 lr=0.0016000001\n",
      "lr00 Iter 11800 , testing: 0.15989481  training :5.1931334e-11 lr=0.0016000001\n",
      "lr00 Iter 11900 , testing: 0.15989484  training :5.165605e-11 lr=0.0016000001\n",
      "lr00 Iter 12000 , testing: 0.15989485  training :5.1729947e-11 lr=0.00032000002\n",
      "lr00 Iter 12100 , testing: 0.15989485  training :5.151313e-11 lr=0.00032000002\n",
      "lr00 Iter 12200 , testing: 0.15989485  training :5.1649556e-11 lr=0.00032000002\n",
      "lr00 Iter 12300 , testing: 0.15989485  training :5.1585403e-11 lr=0.00032000002\n",
      "lr00 Iter 12400 , testing: 0.15989487  training :5.1656863e-11 lr=0.00032000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c1057c107e61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# prediction_value = sess.run(prediction, feed_dict={x:x_data})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mpr_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:400], tba_y[0:400]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 800\n",
    "num_layer_3 = 1\n",
    "num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4.0e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=4000,decay_rate=0.2,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing: 56.310184  training :56.0012 lr=0.05\n",
      "lr00 Iter 100 , testing: 1.7989333  training :1.8210361 lr=0.048\n",
      "lr00 Iter 200 , testing: 1.4164603  training :1.3398721 lr=0.04608\n",
      "lr00 Iter 300 , testing: 0.9351989  training :0.7976977 lr=0.044236798\n",
      "lr00 Iter 400 , testing: 0.58344054  training :0.4337151 lr=0.042467322\n",
      "lr00 Iter 500 , testing: 0.38591835  training :0.24802916 lr=0.04076863\n",
      "lr00 Iter 600 , testing: 0.30344212  training :0.16970776 lr=0.039137885\n",
      "lr00 Iter 700 , testing: 0.24784255  training :0.11940455 lr=0.03757237\n",
      "lr00 Iter 800 , testing: 0.23283076  training :0.113716744 lr=0.036069475\n",
      "lr00 Iter 900 , testing: 0.22028859  training :0.10341653 lr=0.034626693\n",
      "lr00 Iter 1000 , testing: 0.19439758  training :0.08285152 lr=0.033241626\n",
      "lr00 Iter 1100 , testing: 0.18184315  training :0.07337068 lr=0.031911958\n",
      "lr00 Iter 1200 , testing: 0.16677396  training :0.061413493 lr=0.03063548\n",
      "lr00 Iter 1300 , testing: 0.16111912  training :0.056130223 lr=0.029410062\n",
      "lr00 Iter 1400 , testing: 0.1545482  training :0.053840537 lr=0.028233657\n",
      "lr00 Iter 1500 , testing: 0.14942737  training :0.051105954 lr=0.027104309\n",
      "lr00 Iter 1600 , testing: 0.14776741  training :0.047611315 lr=0.026020138\n",
      "lr00 Iter 1700 , testing: 0.14998345  training :0.051464144 lr=0.02497933\n",
      "lr00 Iter 1800 , testing: 0.14584173  training :0.04682083 lr=0.023980157\n",
      "lr00 Iter 1900 , testing: 0.14161493  training :0.04433089 lr=0.023020951\n",
      "lr00 Iter 2000 , testing: 0.1384691  training :0.041097917 lr=0.022100111\n",
      "lr00 Iter 2100 , testing: 0.1326558  training :0.034314968 lr=0.021216108\n",
      "lr00 Iter 2200 , testing: 0.13736653  training :0.038722802 lr=0.020367462\n",
      "lr00 Iter 2300 , testing: 0.131777  training :0.0334549 lr=0.019552764\n",
      "lr00 Iter 2400 , testing: 0.1329398  training :0.03603843 lr=0.018770654\n",
      "lr00 Iter 2500 , testing: 0.13180242  training :0.03533386 lr=0.018019825\n",
      "lr00 Iter 2600 , testing: 0.12616362  training :0.028643036 lr=0.017299032\n",
      "lr00 Iter 2700 , testing: 0.12893473  training :0.032498356 lr=0.01660707\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d574c6bac9da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;31m# sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:1200], tba_y[0:1200]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 1000\n",
    "num_layer_2 = 400\n",
    "num_layer_3 = 1\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 5.0e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=100,decay_rate=0.96,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:0.8})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:1200], tba_y[0:1200]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 1000\n",
    "num_layer_2 = 400\n",
    "num_layer_3 = 1\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1/500.0) \n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2/100.0) \n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 5.0e\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=100,decay_rate=0.96,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:0.7})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing: 47.370186  training :47.43638 lr=0.5\n",
      "lr00 Iter 10 , testing: 36.20884  training :36.25797 lr=0.5\n",
      "lr00 Iter 20 , testing: 17.928661  training :17.953878 lr=0.48\n",
      "lr00 Iter 30 , testing: 2.9377153  training :2.9471793 lr=0.48\n",
      "lr00 Iter 40 , testing: 0.19107772  training :0.20017155 lr=0.4608\n",
      "lr00 Iter 50 , testing: 0.06577702  training :0.07542203 lr=0.4608\n",
      "lr00 Iter 60 , testing: 0.060510032  training :0.07023596 lr=0.44236797\n",
      "lr00 Iter 70 , testing: 0.06016572  training :0.069850564 lr=0.44236797\n",
      "lr00 Iter 80 , testing: 0.060079888  training :0.06969983 lr=0.42467323\n",
      "lr00 Iter 90 , testing: 0.059931293  training :0.069494314 lr=0.42467323\n",
      "lr00 Iter 100 , testing: 0.059821278  training :0.06931738 lr=0.4076863\n",
      "lr00 Iter 110 , testing: 0.059719943  training :0.06914445 lr=0.4076863\n",
      "lr00 Iter 120 , testing: 0.059591115  training :0.068948515 lr=0.39137885\n",
      "lr00 Iter 130 , testing: 0.05957503  training :0.068833426 lr=0.39137885\n",
      "lr00 Iter 140 , testing: 0.05943879  training :0.06860835 lr=0.3757237\n",
      "lr00 Iter 150 , testing: 0.05941946  training :0.068483956 lr=0.3757237\n",
      "lr00 Iter 160 , testing: 0.05916361  training :0.068139546 lr=0.36069474\n",
      "lr00 Iter 170 , testing: 0.058982246  training :0.06785939 lr=0.36069474\n",
      "lr00 Iter 180 , testing: 0.0590928  training :0.067812726 lr=0.34626693\n",
      "lr00 Iter 190 , testing: 0.05873252  training :0.067363255 lr=0.34626693\n",
      "lr00 Iter 200 , testing: 0.058802944  training :0.06728412 lr=0.33241624\n",
      "lr00 Iter 210 , testing: 0.058555417  training :0.06691146 lr=0.33241624\n",
      "lr00 Iter 220 , testing: 0.058305636  training :0.066546865 lr=0.31911957\n",
      "lr00 Iter 230 , testing: 0.058323532  training :0.06641938 lr=0.31911957\n",
      "lr00 Iter 240 , testing: 0.05797657  training :0.065993086 lr=0.3063548\n",
      "lr00 Iter 250 , testing: 0.058245085  training :0.06607699 lr=0.3063548\n",
      "lr00 Iter 260 , testing: 0.05854555  training :0.06623197 lr=0.2941006\n",
      "lr00 Iter 270 , testing: 0.05789615  training :0.06549424 lr=0.2941006\n",
      "lr00 Iter 280 , testing: 0.05742135  training :0.06496196 lr=0.28233656\n",
      "lr00 Iter 290 , testing: 0.05775974  training :0.06511932 lr=0.28233656\n",
      "lr00 Iter 300 , testing: 0.057188664  training :0.06451292 lr=0.2710431\n",
      "lr00 Iter 310 , testing: 0.057202127  training :0.064367205 lr=0.2710431\n",
      "lr00 Iter 320 , testing: 0.056901917  training :0.06398554 lr=0.26020136\n",
      "lr00 Iter 330 , testing: 0.05683066  training :0.063801855 lr=0.26020136\n",
      "lr00 Iter 340 , testing: 0.056803454  training :0.06373029 lr=0.2497933\n",
      "lr00 Iter 350 , testing: 0.056566417  training :0.063383065 lr=0.2497933\n",
      "lr00 Iter 360 , testing: 0.056502648  training :0.06317079 lr=0.23980157\n",
      "lr00 Iter 370 , testing: 0.056375947  training :0.06300446 lr=0.23980157\n",
      "lr00 Iter 380 , testing: 0.05617567  training :0.06267465 lr=0.2302095\n",
      "lr00 Iter 390 , testing: 0.056066513  training :0.062475435 lr=0.2302095\n",
      "lr00 Iter 400 , testing: 0.05609165  training :0.062382255 lr=0.22100112\n",
      "lr00 Iter 410 , testing: 0.055959675  training :0.062163558 lr=0.22100112\n",
      "lr00 Iter 420 , testing: 0.056075085  training :0.06217013 lr=0.21216106\n",
      "lr00 Iter 430 , testing: 0.055938616  training :0.06195098 lr=0.21216106\n",
      "lr00 Iter 440 , testing: 0.055546317  training :0.061527308 lr=0.20367461\n",
      "lr00 Iter 450 , testing: 0.055447906  training :0.061345797 lr=0.20367461\n",
      "lr00 Iter 460 , testing: 0.055347834  training :0.06116668 lr=0.19552763\n",
      "lr00 Iter 470 , testing: 0.05530191  training :0.0610225 lr=0.19552763\n",
      "lr00 Iter 480 , testing: 0.055302948  training :0.06093439 lr=0.18770653\n",
      "lr00 Iter 490 , testing: 0.055165783  training :0.060730692 lr=0.18770653\n",
      "lr00 Iter 500 , testing: 0.055155877  training :0.060638536 lr=0.18019825\n",
      "lr00 Iter 510 , testing: 0.054974493  training :0.06040237 lr=0.18019825\n",
      "lr00 Iter 520 , testing: 0.05503238  training :0.060373705 lr=0.17299032\n",
      "lr00 Iter 530 , testing: 0.054745484  training :0.060048774 lr=0.17299032\n",
      "lr00 Iter 540 , testing: 0.054648027  training :0.059891757 lr=0.1660707\n",
      "lr00 Iter 550 , testing: 0.054568194  training :0.059754252 lr=0.1660707\n",
      "lr00 Iter 560 , testing: 0.05456002  training :0.05965517 lr=0.15942788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-c4a3a6e2b384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# prediction_value = sess.run(prediction, feed_dict={x:x_data})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mpr_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:1200], tba_y[0:1200]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 1000\n",
    "num_layer_2 = 400\n",
    "num_layer_3 = 1\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1/500.0) * 1.0\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2/1.0) * 1.0\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 5.0e-1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=20,decay_rate=0.96,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:0.8})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing: 5.338361  training :5.320376 lr=5.0\n",
      "lr00 Iter 10 , testing: 0.021587603  training :0.023291156 lr=5.0\n",
      "lr00 Iter 20 , testing: 0.020716295  training :0.022211706 lr=4.5\n",
      "lr00 Iter 30 , testing: 0.019956607  training :0.021275893 lr=4.5\n",
      "lr00 Iter 40 , testing: 0.019186322  training :0.020330869 lr=4.0499997\n",
      "lr00 Iter 50 , testing: 0.018423444  training :0.019397935 lr=4.0499997\n",
      "lr00 Iter 60 , testing: 0.017556416  training :0.018340716 lr=3.6449995\n",
      "lr00 Iter 70 , testing: 0.016599325  training :0.017177114 lr=3.6449995\n",
      "lr00 Iter 80 , testing: 0.015395948  training :0.015716072 lr=3.2804995\n",
      "lr00 Iter 90 , testing: 0.017044717  training :0.016392149 lr=3.2804995\n",
      "lr00 Iter 100 , testing: 0.017283773  training :0.017730705 lr=2.9524496\n",
      "lr00 Iter 110 , testing: 0.01580771  training :0.016080577 lr=2.9524496\n",
      "lr00 Iter 120 , testing: 0.01408583  training :0.014149131 lr=2.6572046\n",
      "lr00 Iter 130 , testing: 0.012322365  training :0.01207265 lr=2.6572046\n",
      "lr00 Iter 140 , testing: 0.018060496  training :0.018584017 lr=2.391484\n",
      "lr00 Iter 150 , testing: 0.016334884  training :0.016520122 lr=2.391484\n",
      "lr00 Iter 160 , testing: 0.014291329  training :0.014196991 lr=2.1523356\n",
      "lr00 Iter 170 , testing: 0.012276896  training :0.01174073 lr=2.1523356\n",
      "lr00 Iter 180 , testing: 0.17330752  training :0.16896793 lr=1.937102\n",
      "lr00 Iter 190 , testing: 0.012782444  training :0.012470163 lr=1.937102\n",
      "lr00 Iter 200 , testing: 0.010971366  training :0.010076185 lr=1.7433918\n",
      "lr00 Iter 210 , testing: 0.013530085  training :0.013027425 lr=1.7433918\n",
      "lr00 Iter 220 , testing: 0.011837912  training :0.010398456 lr=1.5690525\n",
      "lr00 Iter 230 , testing: 0.0122459885  training :0.011489628 lr=1.5690525\n",
      "lr00 Iter 240 , testing: 0.044887163  training :0.042725384 lr=1.4121473\n",
      "lr00 Iter 250 , testing: 0.009634  training :0.008644041 lr=1.4121473\n",
      "lr00 Iter 260 , testing: 0.014934894  training :0.014692616 lr=1.2709324\n",
      "lr00 Iter 270 , testing: 0.009814482  training :0.008915495 lr=1.2709324\n",
      "lr00 Iter 280 , testing: 0.006833724  training :0.0055304426 lr=1.1438392\n",
      "lr00 Iter 290 , testing: 0.01065208  training :0.009890552 lr=1.1438392\n",
      "lr00 Iter 300 , testing: 0.007904457  training :0.006791235 lr=1.0294552\n",
      "lr00 Iter 310 , testing: 0.04194827  training :0.038872264 lr=1.0294552\n",
      "lr00 Iter 320 , testing: 0.0096431505  training :0.007894465 lr=0.92650974\n",
      "lr00 Iter 330 , testing: 0.013337757  training :0.0114824185 lr=0.92650974\n",
      "lr00 Iter 340 , testing: 0.020355117  training :0.018001072 lr=0.8338587\n",
      "lr00 Iter 350 , testing: 0.02124081  training :0.018445112 lr=0.8338587\n",
      "lr00 Iter 360 , testing: 0.009005699  training :0.0072221207 lr=0.75047284\n",
      "lr00 Iter 370 , testing: 0.012767396  training :0.01079751 lr=0.75047284\n",
      "lr00 Iter 380 , testing: 0.01753033  training :0.015186242 lr=0.6754255\n",
      "lr00 Iter 390 , testing: 0.018084629  training :0.015392832 lr=0.6754255\n",
      "lr00 Iter 400 , testing: 0.008468169  training :0.0066466224 lr=0.607883\n",
      "lr00 Iter 410 , testing: 0.0150316395  training :0.01270492 lr=0.607883\n",
      "lr00 Iter 420 , testing: 0.01329578  training :0.010984966 lr=0.54709464\n",
      "lr00 Iter 430 , testing: 0.013687721  training :0.011288472 lr=0.54709464\n",
      "lr00 Iter 440 , testing: 0.009218946  training :0.007247358 lr=0.49238518\n",
      "lr00 Iter 450 , testing: 0.014492444  training :0.012008236 lr=0.49238518\n",
      "lr00 Iter 460 , testing: 0.008346245  training :0.0064232987 lr=0.44314665\n",
      "lr00 Iter 470 , testing: 0.012257012  training :0.00991339 lr=0.44314665\n",
      "lr00 Iter 480 , testing: 0.007655854  training :0.00579668 lr=0.39883196\n",
      "lr00 Iter 490 , testing: 0.009227855  training :0.007135611 lr=0.39883196\n",
      "lr00 Iter 500 , testing: 0.007106911  training :0.0053077075 lr=0.35894874\n",
      "lr00 Iter 510 , testing: 0.006108872  training :0.004392048 lr=0.35894874\n",
      "lr00 Iter 520 , testing: 0.007357359  training :0.005515917 lr=0.32305387\n",
      "lr00 Iter 530 , testing: 0.004692981  training :0.0032336514 lr=0.32305387\n",
      "lr00 Iter 540 , testing: 0.008058544  training :0.006125168 lr=0.29074848\n",
      "lr00 Iter 550 , testing: 0.004656602  training :0.003200058 lr=0.29074848\n",
      "lr00 Iter 560 , testing: 0.0066886926  training :0.004920867 lr=0.26167363\n",
      "lr00 Iter 570 , testing: 0.004555682  training :0.0031191173 lr=0.26167363\n",
      "lr00 Iter 580 , testing: 0.00571742  training :0.004090136 lr=0.23550627\n",
      "lr00 Iter 590 , testing: 0.0041975314  training :0.0028387073 lr=0.23550627\n",
      "lr00 Iter 600 , testing: 0.004991495  training :0.0034859646 lr=0.21195562\n",
      "lr00 Iter 610 , testing: 0.003955342  training :0.002653375 lr=0.21195562\n",
      "lr00 Iter 620 , testing: 0.0044727405  training :0.003064997 lr=0.19076006\n",
      "lr00 Iter 630 , testing: 0.0037096126  training :0.0024684593 lr=0.19076006\n",
      "lr00 Iter 640 , testing: 0.0040629446  training :0.00274018 lr=0.17168404\n",
      "lr00 Iter 650 , testing: 0.0035229153  training :0.0023301514 lr=0.17168404\n",
      "lr00 Iter 660 , testing: 0.0037835685  training :0.002523448 lr=0.15451564\n",
      "lr00 Iter 670 , testing: 0.003376716  training :0.0022224397 lr=0.15451564\n",
      "lr00 Iter 680 , testing: 0.0035716882  training :0.0023613393 lr=0.13906407\n",
      "lr00 Iter 690 , testing: 0.0032560322  training :0.002133861 lr=0.13906407\n",
      "lr00 Iter 700 , testing: 0.0033999728  training :0.002231521 lr=0.12515765\n",
      "lr00 Iter 710 , testing: 0.003153215  training :0.0020588462 lr=0.12515765\n",
      "lr00 Iter 720 , testing: 0.0032572704  training :0.0021250413 lr=0.11264189\n",
      "lr00 Iter 730 , testing: 0.0030660192  training :0.0019956105 lr=0.11264189\n",
      "lr00 Iter 740 , testing: 0.00314093  training :0.0020393124 lr=0.101377696\n",
      "lr00 Iter 750 , testing: 0.0029938987  training :0.001943294 lr=0.101377696\n",
      "lr00 Iter 760 , testing: 0.0030483566  training :0.00197165 lr=0.09123992\n",
      "lr00 Iter 770 , testing: 0.0029344212  training :0.0018998949 lr=0.09123992\n",
      "lr00 Iter 780 , testing: 0.0029732327  training :0.0019170719 lr=0.08211593\n",
      "lr00 Iter 790 , testing: 0.002884009  training :0.001863001 lr=0.08211593\n",
      "lr00 Iter 800 , testing: 0.0029100617  training :0.0018716286 lr=0.073904335\n",
      "lr00 Iter 810 , testing: 0.0028404428  training :0.001831184 lr=0.073904335\n",
      "lr00 Iter 820 , testing: 0.0028566692  training :0.0018336793 lr=0.066513896\n",
      "lr00 Iter 830 , testing: 0.0028033853  training :0.001803999 lr=0.066513896\n",
      "lr00 Iter 840 , testing: 0.0028130477  training :0.0018027233 lr=0.05986251\n",
      "lr00 Iter 850 , testing: 0.0027725857  training :0.0017809394 lr=0.05986251\n",
      "lr00 Iter 860 , testing: 0.0027780274  training :0.0017775615 lr=0.05387626\n",
      "lr00 Iter 870 , testing: 0.0027468745  training :0.00176118 lr=0.05387626\n",
      "lr00 Iter 880 , testing: 0.0027491231  training :0.0017565506 lr=0.04848863\n",
      "lr00 Iter 890 , testing: 0.0027246922  training :0.0017439313 lr=0.04848863\n",
      "lr00 Iter 900 , testing: 0.0027243786  training :0.0017385889 lr=0.043639764\n",
      "lr00 Iter 910 , testing: 0.0027055417  training :0.0017288435 lr=0.043639764\n",
      "lr00 Iter 920 , testing: 0.0027038816  training :0.00172339 lr=0.039275788\n",
      "lr00 Iter 930 , testing: 0.0026896985  training :0.0017157336 lr=0.039275788\n",
      "lr00 Iter 940 , testing: 0.0026880743  training :0.0017106403 lr=0.035348207\n",
      "lr00 Iter 950 , testing: 0.002677211  training :0.0017043311 lr=0.035348207\n",
      "lr00 Iter 960 , testing: 0.0026763915  training :0.0016998161 lr=0.031813387\n",
      "lr00 Iter 970 , testing: 0.0026670718  training :0.001694287 lr=0.031813387\n",
      "lr00 Iter 980 , testing: 0.002666105  training :0.0016901776 lr=0.028632049\n",
      "lr00 Iter 990 , testing: 0.002657079  training :0.0016851964 lr=0.028632049\n",
      "lr00 Iter 1000 , testing: 0.0026548395  training :0.0016812438 lr=0.02576884\n",
      "lr00 Iter 1010 , testing: 0.0026472893  training :0.00167701 lr=0.02576884\n",
      "lr00 Iter 1020 , testing: 0.0026449335  training :0.0016733782 lr=0.023191959\n",
      "lr00 Iter 1030 , testing: 0.0026396492  training :0.0016698086 lr=0.023191959\n",
      "lr00 Iter 1040 , testing: 0.0026388443  training :0.0016666645 lr=0.020872762\n",
      "lr00 Iter 1050 , testing: 0.0026349763  training :0.0016635256 lr=0.020872762\n",
      "lr00 Iter 1060 , testing: 0.0026350718  training :0.0016608667 lr=0.018785484\n",
      "lr00 Iter 1070 , testing: 0.0026292284  training :0.0016577813 lr=0.018785484\n",
      "lr00 Iter 1080 , testing: 0.002627018  training :0.0016551208 lr=0.016906936\n",
      "lr00 Iter 1090 , testing: 0.0026219548  training :0.0016524987 lr=0.016906936\n",
      "lr00 Iter 1100 , testing: 0.0026195396  training :0.0016500774 lr=0.015216242\n",
      "lr00 Iter 1110 , testing: 0.0026166304  training :0.0016478624 lr=0.015216242\n",
      "lr00 Iter 1120 , testing: 0.0026151948  training :0.0016457032 lr=0.013694616\n",
      "lr00 Iter 1130 , testing: 0.0026140248  training :0.0016437556 lr=0.013694616\n",
      "lr00 Iter 1140 , testing: 0.0026153317  training :0.0016420363 lr=0.012325155\n",
      "lr00 Iter 1150 , testing: 0.0026120413  training :0.0016401266 lr=0.012325155\n",
      "lr00 Iter 1160 , testing: 0.0026105472  training :0.0016383859 lr=0.011092639\n",
      "lr00 Iter 1170 , testing: 0.0026067076  training :0.0016366777 lr=0.011092639\n",
      "lr00 Iter 1180 , testing: 0.002604796  training :0.001635092 lr=0.009983376\n",
      "lr00 Iter 1190 , testing: 0.002602931  training :0.0016336555 lr=0.009983376\n",
      "lr00 Iter 1200 , testing: 0.0026015772  training :0.0016322351 lr=0.008985037\n",
      "lr00 Iter 1210 , testing: 0.0026003418  training :0.0016309455 lr=0.008985037\n",
      "lr00 Iter 1220 , testing: 0.0025992144  training :0.0016296665 lr=0.008086533\n",
      "lr00 Iter 1230 , testing: 0.0025982184  training :0.0016285031 lr=0.008086533\n",
      "lr00 Iter 1240 , testing: 0.0025981427  training :0.0016273627 lr=0.00727788\n",
      "lr00 Iter 1250 , testing: 0.002598238  training :0.001626354 lr=0.00727788\n",
      "lr00 Iter 1260 , testing: 0.0025975453  training :0.0016253324 lr=0.006550092\n",
      "lr00 Iter 1270 , testing: 0.002595107  training :0.0016243347 lr=0.006550092\n",
      "lr00 Iter 1280 , testing: 0.0025938477  training :0.0016233962 lr=0.005895082\n",
      "lr00 Iter 1290 , testing: 0.0025927434  training :0.0016225434 lr=0.005895082\n",
      "lr00 Iter 1300 , testing: 0.0025919527  training :0.0016217025 lr=0.0053055743\n",
      "lr00 Iter 1310 , testing: 0.00259126  training :0.00162094 lr=0.0053055743\n",
      "lr00 Iter 1320 , testing: 0.002590577  training :0.0016201817 lr=0.004775016\n",
      "lr00 Iter 1330 , testing: 0.002589955  training :0.0016194923 lr=0.004775016\n",
      "lr00 Iter 1340 , testing: 0.0025893436  training :0.0016188084 lr=0.0042975144\n",
      "lr00 Iter 1350 , testing: 0.002588786  training :0.0016181871 lr=0.0042975144\n",
      "lr00 Iter 1360 , testing: 0.0025882367  training :0.0016175716 lr=0.003867763\n",
      "lr00 Iter 1370 , testing: 0.0025877315  training :0.0016170113 lr=0.003867763\n",
      "lr00 Iter 1380 , testing: 0.0025872395  training :0.0016164564 lr=0.0034809867\n",
      "lr00 Iter 1390 , testing: 0.0025867822  training :0.0016159514 lr=0.0034809867\n",
      "lr00 Iter 1400 , testing: 0.0025863398  training :0.0016154542 lr=0.0031328879\n",
      "lr00 Iter 1410 , testing: 0.0025859324  training :0.0016149969 lr=0.0031328879\n",
      "lr00 Iter 1420 , testing: 0.002585533  training :0.0016145487 lr=0.0028195991\n",
      "lr00 Iter 1430 , testing: 0.0025851687  training :0.0016141409 lr=0.0028195991\n",
      "lr00 Iter 1440 , testing: 0.002584815  training :0.0016137377 lr=0.0025376393\n",
      "lr00 Iter 1450 , testing: 0.0025844874  training :0.001613369 lr=0.0025376393\n",
      "lr00 Iter 1460 , testing: 0.0025841605  training :0.0016130061 lr=0.0022838751\n",
      "lr00 Iter 1470 , testing: 0.0025838697  training :0.0016126747 lr=0.0022838751\n",
      "lr00 Iter 1480 , testing: 0.0025835752  training :0.0016123458 lr=0.0020554876\n",
      "lr00 Iter 1490 , testing: 0.002583312  training :0.0016120495 lr=0.0020554876\n",
      "lr00 Iter 1500 , testing: 0.0025830516  training :0.0016117551 lr=0.0018499386\n",
      "lr00 Iter 1510 , testing: 0.002582812  training :0.0016114885 lr=0.0018499386\n",
      "lr00 Iter 1520 , testing: 0.0025825764  training :0.0016112243 lr=0.0016649449\n",
      "lr00 Iter 1530 , testing: 0.0025823624  training :0.0016109833 lr=0.0016649449\n",
      "lr00 Iter 1540 , testing: 0.0025821493  training :0.0016107457 lr=0.0014984503\n",
      "lr00 Iter 1550 , testing: 0.0025819498  training :0.0016105291 lr=0.0014984503\n",
      "lr00 Iter 1560 , testing: 0.0025817628  training :0.0016103117 lr=0.0013486053\n",
      "lr00 Iter 1570 , testing: 0.0025815903  training :0.0016101189 lr=0.0013486053\n",
      "lr00 Iter 1580 , testing: 0.002581423  training :0.001609923 lr=0.0012137447\n",
      "lr00 Iter 1590 , testing: 0.002581263  training :0.0016097466 lr=0.0012137447\n",
      "lr00 Iter 1600 , testing: 0.0025811119  training :0.0016095756 lr=0.0010923703\n",
      "lr00 Iter 1610 , testing: 0.0025809782  training :0.0016094199 lr=0.0010923703\n",
      "lr00 Iter 1620 , testing: 0.002580841  training :0.001609264 lr=0.0009831331\n",
      "lr00 Iter 1630 , testing: 0.0025807156  training :0.0016091246 lr=0.0009831331\n",
      "lr00 Iter 1640 , testing: 0.0025805933  training :0.0016089857 lr=0.00088481983\n",
      "lr00 Iter 1650 , testing: 0.0025804802  training :0.0016088602 lr=0.00088481983\n",
      "lr00 Iter 1660 , testing: 0.0025803728  training :0.0016087331 lr=0.0007963378\n",
      "lr00 Iter 1670 , testing: 0.0025802702  training :0.001608622 lr=0.0007963378\n",
      "lr00 Iter 1680 , testing: 0.002580173  training :0.0016085111 lr=0.000716704\n",
      "lr00 Iter 1690 , testing: 0.002580083  training :0.0016084118 lr=0.000716704\n",
      "lr00 Iter 1700 , testing: 0.0025799945  training :0.0016083107 lr=0.00064503355\n",
      "lr00 Iter 1710 , testing: 0.0025799198  training :0.0016082203 lr=0.00064503355\n",
      "lr00 Iter 1720 , testing: 0.0025798366  training :0.0016081299 lr=0.0005805302\n",
      "lr00 Iter 1730 , testing: 0.0025797654  training :0.0016080495 lr=0.0005805302\n",
      "lr00 Iter 1740 , testing: 0.0025796962  training :0.0016079696 lr=0.00052247714\n",
      "lr00 Iter 1750 , testing: 0.0025796385  training :0.0016078989 lr=0.00052247714\n",
      "lr00 Iter 1760 , testing: 0.0025795777  training :0.0016078262 lr=0.00047022945\n",
      "lr00 Iter 1770 , testing: 0.0025795214  training :0.0016077642 lr=0.00047022945\n",
      "lr00 Iter 1780 , testing: 0.0025794657  training :0.0016077018 lr=0.00042320648\n",
      "lr00 Iter 1790 , testing: 0.0025794194  training :0.0016076437 lr=0.00042320648\n",
      "lr00 Iter 1800 , testing: 0.0025793745  training :0.001607588 lr=0.00038088585\n",
      "lr00 Iter 1810 , testing: 0.0025793337  training :0.0016075387 lr=0.00038088585\n",
      "lr00 Iter 1820 , testing: 0.002579291  training :0.0016074901 lr=0.00034279726\n",
      "lr00 Iter 1830 , testing: 0.0025792478  training :0.0016074453 lr=0.00034279726\n",
      "lr00 Iter 1840 , testing: 0.002579206  training :0.0016074023 lr=0.0003085175\n",
      "lr00 Iter 1850 , testing: 0.0025791721  training :0.0016073631 lr=0.0003085175\n",
      "lr00 Iter 1860 , testing: 0.0025791356  training :0.0016073255 lr=0.00027766573\n",
      "lr00 Iter 1870 , testing: 0.0025791065  training :0.0016072912 lr=0.00027766573\n",
      "lr00 Iter 1880 , testing: 0.0025790792  training :0.0016072581 lr=0.00024989917\n",
      "lr00 Iter 1890 , testing: 0.0025790515  training :0.0016072276 lr=0.00024989917\n",
      "lr00 Iter 1900 , testing: 0.0025790238  training :0.0016071991 lr=0.00022490925\n",
      "lr00 Iter 1910 , testing: 0.0025789982  training :0.0016071721 lr=0.00022490925\n",
      "lr00 Iter 1920 , testing: 0.0025789754  training :0.0016071463 lr=0.00020241832\n",
      "lr00 Iter 1930 , testing: 0.0025789614  training :0.0016071224 lr=0.00020241832\n",
      "lr00 Iter 1940 , testing: 0.002578943  training :0.0016071014 lr=0.00018217647\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6fefe537c50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;31m# sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(t0ba_X[0:1200], t0ba_y[0:1200]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 500\n",
    "num_layer_2 = 200\n",
    "num_layer_3 = 1\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1/1.0) * 1.0\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2/1.0) * 1.0\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "# loss = tf.reduce_mean(tf.abs(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 5.0e0\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=20,decay_rate=0.9,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing: 60.30573  training :61.675926 lr=0.04\n",
      "lr00 Iter 100 , testing: 0.4766513  training :0.46910778 lr=0.04\n",
      "lr00 Iter 200 , testing: 0.27740556  training :0.25755876 lr=0.04\n",
      "lr00 Iter 300 , testing: 0.20957595  training :0.11473274 lr=0.04\n",
      "lr00 Iter 400 , testing: 0.18435393  training :0.04474062 lr=0.04\n",
      "lr00 Iter 500 , testing: 0.184184  training :0.025478868 lr=0.04\n",
      "lr00 Iter 600 , testing: 0.18363227  training :0.017280933 lr=0.04\n",
      "lr00 Iter 700 , testing: 0.18186119  training :0.012915393 lr=0.04\n",
      "lr00 Iter 800 , testing: 0.17959751  training :0.010412722 lr=0.04\n",
      "lr00 Iter 900 , testing: 0.17721468  training :0.0089031495 lr=0.04\n",
      "lr00 Iter 1000 , testing: 0.17360589  training :0.0007143355 lr=0.016\n",
      "lr00 Iter 1100 , testing: 0.1744571  training :0.00027438876 lr=0.016\n",
      "lr00 Iter 1200 , testing: 0.17470202  training :0.00034536704 lr=0.016\n",
      "lr00 Iter 1300 , testing: 0.1746418  training :0.00073259335 lr=0.016\n",
      "lr00 Iter 1400 , testing: 0.1745215  training :0.0010511592 lr=0.016\n",
      "lr00 Iter 1500 , testing: 0.17429838  training :0.0012730319 lr=0.016\n",
      "lr00 Iter 1600 , testing: 0.1740003  training :0.0014127549 lr=0.016\n",
      "lr00 Iter 1700 , testing: 0.17365517  training :0.0014935224 lr=0.016\n",
      "lr00 Iter 1800 , testing: 0.17328285  training :0.0015348475 lr=0.016\n",
      "lr00 Iter 1900 , testing: 0.17289622  training :0.0015503834 lr=0.016\n",
      "lr00 Iter 2000 , testing: 0.17365105  training :6.883785e-05 lr=0.0064000003\n",
      "lr00 Iter 2100 , testing: 0.17354292  training :7.459202e-06 lr=0.0064000003\n",
      "lr00 Iter 2200 , testing: 0.17368983  training :5.5231226e-06 lr=0.0064000003\n",
      "lr00 Iter 2300 , testing: 0.17379393  training :4.4768035e-06 lr=0.0064000003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8e93dc3fcbcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# prediction_value = sess.run(prediction, feed_dict={x:x_data})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mpr_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:800], tba_y[0:800]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 800\n",
    "num_layer_3 = 1\n",
    "num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4.0e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=1000,decay_rate=0.4,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr00 Iter 0 , testing: 6.3804116  training :6.4279704 lr=0.4\n",
      "lr00 Iter 100 , testing: 0.34423274  training :0.17627607 lr=0.4\n",
      "lr00 Iter 200 , testing: 0.29442635  training :0.0893424 lr=0.4\n",
      "lr00 Iter 300 , testing: 0.2821008  training :0.080039576 lr=0.4\n",
      "lr00 Iter 400 , testing: 0.2839573  training :0.06890509 lr=0.4\n",
      "lr00 Iter 500 , testing: 0.2883313  training :0.07297411 lr=0.4\n",
      "lr00 Iter 600 , testing: 0.28278112  training :0.06069554 lr=0.4\n",
      "lr00 Iter 700 , testing: 0.28328592  training :0.05613925 lr=0.4\n",
      "lr00 Iter 800 , testing: 0.28733602  training :0.06249797 lr=0.4\n",
      "lr00 Iter 900 , testing: 0.28543764  training :0.051287603 lr=0.4\n",
      "lr00 Iter 1000 , testing: 0.30750954  training :0.043484226 lr=0.2\n",
      "lr00 Iter 1100 , testing: 0.29457125  training :0.025351668 lr=0.2\n",
      "lr00 Iter 1200 , testing: 0.29371828  training :0.03189993 lr=0.2\n",
      "lr00 Iter 1300 , testing: 0.29664078  training :0.024936616 lr=0.2\n",
      "lr00 Iter 1400 , testing: 0.29619563  training :0.026027074 lr=0.2\n",
      "lr00 Iter 1500 , testing: 0.2972465  training :0.024711229 lr=0.2\n",
      "lr00 Iter 1600 , testing: 0.29910436  training :0.021010647 lr=0.2\n",
      "lr00 Iter 1700 , testing: 0.29737732  training :0.027590519 lr=0.2\n",
      "lr00 Iter 1800 , testing: 0.29698548  training :0.031786345 lr=0.2\n",
      "lr00 Iter 1900 , testing: 0.29653078  training :0.032563824 lr=0.2\n",
      "lr00 Iter 2000 , testing: 0.30789334  training :0.011569774 lr=0.1\n",
      "lr00 Iter 2100 , testing: 0.3034422  training :0.013128928 lr=0.1\n",
      "lr00 Iter 2200 , testing: 0.30339515  training :0.014581948 lr=0.1\n",
      "lr00 Iter 2300 , testing: 0.30325326  training :0.014899537 lr=0.1\n",
      "lr00 Iter 2400 , testing: 0.303696  training :0.014754456 lr=0.1\n",
      "lr00 Iter 2500 , testing: 0.30358824  training :0.015516125 lr=0.1\n",
      "lr00 Iter 2600 , testing: 0.30392647  training :0.0145708285 lr=0.1\n",
      "lr00 Iter 2700 , testing: 0.30356625  training :0.0157219 lr=0.1\n",
      "lr00 Iter 2800 , testing: 0.30364457  training :0.015813598 lr=0.1\n",
      "lr00 Iter 2900 , testing: 0.30401233  training :0.015285138 lr=0.1\n",
      "lr00 Iter 3000 , testing: 0.3115844  training :0.013806754 lr=0.05\n",
      "lr00 Iter 3100 , testing: 0.31566483  training :0.011831718 lr=0.05\n",
      "lr00 Iter 3200 , testing: 0.3156976  training :0.011806192 lr=0.05\n",
      "lr00 Iter 3300 , testing: 0.31532174  training :0.011634529 lr=0.05\n",
      "lr00 Iter 3400 , testing: 0.31540838  training :0.011755497 lr=0.05\n",
      "lr00 Iter 3500 , testing: 0.31546083  training :0.010900553 lr=0.05\n",
      "lr00 Iter 3600 , testing: 0.3157224  training :0.011905411 lr=0.05\n",
      "lr00 Iter 3700 , testing: 0.31676  training :0.012568731 lr=0.05\n",
      "lr00 Iter 3800 , testing: 0.315698  training :0.011843398 lr=0.05\n",
      "lr00 Iter 3900 , testing: 0.31610018  training :0.011889461 lr=0.05\n",
      "lr00 Iter 4000 , testing: 0.311156  training :0.006580988 lr=0.025\n",
      "lr00 Iter 4100 , testing: 0.30931085  training :0.003798754 lr=0.025\n",
      "lr00 Iter 4200 , testing: 0.3094146  training :0.0038307752 lr=0.025\n",
      "lr00 Iter 4300 , testing: 0.3091698  training :0.0041512283 lr=0.025\n",
      "lr00 Iter 4400 , testing: 0.30913648  training :0.0041349563 lr=0.025\n",
      "lr00 Iter 4500 , testing: 0.3091784  training :0.004342844 lr=0.025\n",
      "lr00 Iter 4600 , testing: 0.3090402  training :0.0043436764 lr=0.025\n",
      "lr00 Iter 4700 , testing: 0.30914867  training :0.0043243375 lr=0.025\n",
      "lr00 Iter 4800 , testing: 0.3091057  training :0.0043793847 lr=0.025\n",
      "lr00 Iter 4900 , testing: 0.309257  training :0.004133729 lr=0.025\n",
      "lr00 Iter 5000 , testing: 0.31126302  training :0.0038651654 lr=0.0125\n",
      "lr00 Iter 5100 , testing: 0.312673  training :0.003593191 lr=0.0125\n",
      "lr00 Iter 5200 , testing: 0.3126133  training :0.0036363262 lr=0.0125\n",
      "lr00 Iter 5300 , testing: 0.3125827  training :0.0035976751 lr=0.0125\n",
      "lr00 Iter 5400 , testing: 0.3125969  training :0.00358455 lr=0.0125\n",
      "lr00 Iter 5500 , testing: 0.31258896  training :0.0035752875 lr=0.0125\n",
      "lr00 Iter 5600 , testing: 0.31260675  training :0.0035572068 lr=0.0125\n",
      "lr00 Iter 5700 , testing: 0.3125939  training :0.003548595 lr=0.0125\n",
      "lr00 Iter 5800 , testing: 0.3126351  training :0.0035053014 lr=0.0125\n",
      "lr00 Iter 5900 , testing: 0.31258154  training :0.0036916595 lr=0.0125\n",
      "lr00 Iter 6000 , testing: 0.3112477  training :0.0020255174 lr=0.00625\n",
      "lr00 Iter 6100 , testing: 0.31200632  training :0.0019485593 lr=0.00625\n",
      "lr00 Iter 6200 , testing: 0.31195927  training :0.0019291844 lr=0.00625\n",
      "lr00 Iter 6300 , testing: 0.31197038  training :0.0019497889 lr=0.00625\n",
      "lr00 Iter 6400 , testing: 0.31197172  training :0.0019536666 lr=0.00625\n",
      "lr00 Iter 6500 , testing: 0.31195438  training :0.0019608752 lr=0.00625\n",
      "lr00 Iter 6600 , testing: 0.3119564  training :0.0019525426 lr=0.00625\n",
      "lr00 Iter 6700 , testing: 0.31197616  training :0.0019061991 lr=0.00625\n",
      "lr00 Iter 6800 , testing: 0.31193694  training :0.0019206932 lr=0.00625\n",
      "lr00 Iter 6900 , testing: 0.3119482  training :0.00197008 lr=0.00625\n",
      "lr00 Iter 7000 , testing: 0.31125775  training :0.0010583691 lr=0.003125\n",
      "lr00 Iter 7100 , testing: 0.31091592  training :0.00054511684 lr=0.003125\n",
      "lr00 Iter 7200 , testing: 0.31089538  training :0.0005699549 lr=0.003125\n",
      "lr00 Iter 7300 , testing: 0.31087932  training :0.0005824021 lr=0.003125\n",
      "lr00 Iter 7400 , testing: 0.31087115  training :0.00058254175 lr=0.003125\n",
      "lr00 Iter 7500 , testing: 0.31086552  training :0.00059522904 lr=0.003125\n",
      "lr00 Iter 7600 , testing: 0.3108705  training :0.0005706889 lr=0.003125\n",
      "lr00 Iter 7700 , testing: 0.31085727  training :0.0006000434 lr=0.003125\n",
      "lr00 Iter 7800 , testing: 0.3108473  training :0.00061717373 lr=0.003125\n",
      "lr00 Iter 7900 , testing: 0.31083852  training :0.0006184101 lr=0.003125\n",
      "lr00 Iter 8000 , testing: 0.31120315  training :0.0005147338 lr=0.0015625\n",
      "lr00 Iter 8100 , testing: 0.31134504  training :0.000505897 lr=0.0015625\n",
      "lr00 Iter 8200 , testing: 0.31136698  training :0.0005223308 lr=0.0015625\n",
      "lr00 Iter 8300 , testing: 0.3113676  training :0.0005300556 lr=0.0015625\n",
      "lr00 Iter 8400 , testing: 0.31136838  training :0.0005373512 lr=0.0015625\n",
      "lr00 Iter 8500 , testing: 0.31136638  training :0.0005369987 lr=0.0015625\n",
      "lr00 Iter 8600 , testing: 0.3113634  training :0.00053635833 lr=0.0015625\n",
      "lr00 Iter 8700 , testing: 0.3113653  training :0.0005372235 lr=0.0015625\n",
      "lr00 Iter 8800 , testing: 0.31136438  training :0.00053963833 lr=0.0015625\n",
      "lr00 Iter 8900 , testing: 0.3113638  training :0.0005447592 lr=0.0015625\n",
      "lr00 Iter 9000 , testing: 0.3111772  training :0.00027046885 lr=0.00078125\n",
      "lr00 Iter 9100 , testing: 0.31106302  training :0.0001448512 lr=0.00078125\n",
      "lr00 Iter 9200 , testing: 0.31106967  training :0.00015519517 lr=0.00078125\n",
      "lr00 Iter 9300 , testing: 0.31106403  training :0.00016074351 lr=0.00078125\n",
      "lr00 Iter 9400 , testing: 0.31106284  training :0.00016228983 lr=0.00078125\n",
      "lr00 Iter 9500 , testing: 0.31106263  training :0.00016123567 lr=0.00078125\n",
      "lr00 Iter 9600 , testing: 0.31106135  training :0.00016303913 lr=0.00078125\n",
      "lr00 Iter 9700 , testing: 0.31106055  training :0.00016380378 lr=0.00078125\n",
      "lr00 Iter 9800 , testing: 0.31105983  training :0.00016388894 lr=0.00078125\n",
      "lr00 Iter 9900 , testing: 0.311059  training :0.00016419718 lr=0.00078125\n",
      "lr00 Iter 10000 , testing: 0.3111554  training :0.0001357317 lr=0.000390625\n",
      "lr00 Iter 10100 , testing: 0.3110851  training :9.764774e-05 lr=0.000390625\n",
      "lr00 Iter 10200 , testing: 0.31109565  training :7.979529e-05 lr=0.000390625\n",
      "lr00 Iter 10300 , testing: 0.31109342  training :8.3652565e-05 lr=0.000390625\n",
      "lr00 Iter 10400 , testing: 0.31109315  training :8.393185e-05 lr=0.000390625\n",
      "lr00 Iter 10500 , testing: 0.3110928  training :8.419752e-05 lr=0.000390625\n",
      "lr00 Iter 10600 , testing: 0.3110925  training :8.446319e-05 lr=0.000390625\n",
      "lr00 Iter 10700 , testing: 0.31109223  training :8.475951e-05 lr=0.000390625\n",
      "lr00 Iter 10800 , testing: 0.31109193  training :8.5003034e-05 lr=0.000390625\n",
      "lr00 Iter 10900 , testing: 0.31109154  training :8.5370884e-05 lr=0.000390625\n",
      "lr00 Iter 11000 , testing: 0.31114113  training :6.88604e-05 lr=0.0001953125\n",
      "lr00 Iter 11100 , testing: 0.31110987  training :3.3841814e-05 lr=0.0001953125\n",
      "lr00 Iter 11200 , testing: 0.31110942  training :4.070827e-05 lr=0.0001953125\n",
      "lr00 Iter 11300 , testing: 0.3111079  training :4.258837e-05 lr=0.0001953125\n",
      "lr00 Iter 11400 , testing: 0.3111077  training :4.2791027e-05 lr=0.0001953125\n",
      "lr00 Iter 11500 , testing: 0.31110764  training :4.301412e-05 lr=0.0001953125\n",
      "lr00 Iter 11600 , testing: 0.31110746  training :4.3177606e-05 lr=0.0001953125\n",
      "lr00 Iter 11700 , testing: 0.31110737  training :4.3325766e-05 lr=0.0001953125\n",
      "lr00 Iter 11800 , testing: 0.31110725  training :4.3533528e-05 lr=0.0001953125\n",
      "lr00 Iter 11900 , testing: 0.31110713  training :4.3661254e-05 lr=0.0001953125\n",
      "lr00 Iter 12000 , testing: 0.31113258  training :3.4664357e-05 lr=9.765625e-05\n",
      "lr00 Iter 12100 , testing: 0.3111142  training :2.1394662e-05 lr=9.765625e-05\n",
      "lr00 Iter 12200 , testing: 0.31111538  training :2.0347323e-05 lr=9.765625e-05\n",
      "lr00 Iter 12300 , testing: 0.3111148  training :2.0958696e-05 lr=9.765625e-05\n",
      "lr00 Iter 12400 , testing: 0.31111425  training :2.154282e-05 lr=9.765625e-05\n",
      "lr00 Iter 12500 , testing: 0.31111416  training :2.1801676e-05 lr=9.765625e-05\n",
      "lr00 Iter 12600 , testing: 0.31111416  training :2.171823e-05 lr=9.765625e-05\n",
      "lr00 Iter 12700 , testing: 0.3111141  training :2.1813597e-05 lr=9.765625e-05\n",
      "lr00 Iter 12800 , testing: 0.31111404  training :2.1970272e-05 lr=9.765625e-05\n",
      "lr00 Iter 12900 , testing: 0.311114  training :2.2055421e-05 lr=9.765625e-05\n",
      "lr00 Iter 13000 , testing: 0.31112686  training :1.6960075e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13100 , testing: 0.3111178  training :1.0531289e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13200 , testing: 0.3111184  training :1.0405268e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13300 , testing: 0.31111795  training :1.0999612e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13400 , testing: 0.31111795  training :1.125506e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13500 , testing: 0.31111786  training :1.12703865e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13600 , testing: 0.31111786  training :1.1341913e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13700 , testing: 0.31111786  training :1.1372566e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13800 , testing: 0.31111786  training :1.140322e-05 lr=4.8828126e-05\n",
      "lr00 Iter 13900 , testing: 0.31111783  training :1.1486666e-05 lr=4.8828126e-05\n",
      "lr00 Iter 14000 , testing: 0.31112438  training :8.768695e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14100 , testing: 0.31112525  training :7.3194506e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14200 , testing: 0.3111266  training :9.368147e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14300 , testing: 0.31112632  training :9.162085e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14400 , testing: 0.31112632  training :9.088857e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14500 , testing: 0.31112632  training :9.093966e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14600 , testing: 0.31112632  training :9.168897e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14700 , testing: 0.31112632  training :9.126323e-06 lr=2.4414063e-05\n",
      "lr00 Iter 14800 , testing: 0.31112638  training :9.116105e-06 lr=2.4414063e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tba_X[0:400], tba_y[0:400]/1.0e8, test_size=0.3) #分开学习和测试的数据\n",
    "y_train = y_train.reshape([y_train.__len__(), 1])\n",
    "y_test = y_test.reshape([y_test.__len__(), 1])\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 800\n",
    "num_layer_3 = 1\n",
    "num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 第二个中间层， num_layer_1 入， num_layer_2 出\n",
    "Weight_L2 = tf.Variable( tf.truncated_normal([num_layer_1, num_layer_2], stddev=0.1))\n",
    "biases_L2 = tf.Variable((tf.zeros([1, num_layer_2]) + 0.1))\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop, Weight_L2) + biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 第三个中间层， num_layer_2 入， num_layer_3 出\n",
    "Weight_L3 = tf.Variable( tf.truncated_normal([num_layer_2, num_layer_3], stddev=0.1))\n",
    "biases_L3 = tf.Variable((tf.zeros([1, num_layer_3]) + 0.1))\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop, Weight_L3) + biases_L3\n",
    "\n",
    "\n",
    "# prediction = tf.nn.softplus(Wx_plus_b_L2)\n",
    "prediction = Wx_plus_b_L3\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss = tf.reduce_mean(tf.abs(y - prediction))\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4.0e-1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=1000,decay_rate=0.5,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864354781.869531"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(tba_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604305878.9369228"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(tba_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 13.,  58., 222., 611., 993., 962., 602., 260.,  67.,  12.]),\n",
       " array([5.88848827e+08, 5.97353994e+08, 6.05859161e+08, 6.14364328e+08,\n",
       "        6.22869495e+08, 6.31374663e+08, 6.39879830e+08, 6.48384997e+08,\n",
       "        6.56890164e+08, 6.65395332e+08, 6.73900499e+08]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(t6ba_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
