{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.learning_curve import validation_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# global variable path\n",
    "MY_PATH = 'E:/pythondata/'\n",
    "# MY_PATH = 'C:/Users/17613/Documents/Calculations/calculations cloud/Python/temp_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n0_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t0ba_X = tba_X\n",
    "t0ba_y = tba_y\n",
    "tv = np.mean(t0ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n8em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t1ba_X = tba_X\n",
    "t1ba_y = tba_y\n",
    "tv = np.mean(t1ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n4em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t2ba_X = tba_X\n",
    "t2ba_y = tba_y\n",
    "tv = np.mean(t2ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n2em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t3ba_X = tba_X\n",
    "t3ba_y = tba_y\n",
    "tv = np.mean(t3ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n1em5_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t4ba_X = tba_X\n",
    "t4ba_y = tba_y\n",
    "tv = np.mean(t4ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n8em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t5ba_X = tba_X\n",
    "t5ba_y = tba_y\n",
    "tv = np.mean(t5ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n7em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t6ba_X = tba_X\n",
    "t6ba_y = tba_y\n",
    "tv = np.mean(t6ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n2em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t7ba_X = tba_X\n",
    "t7ba_y = tba_y\n",
    "tv = np.mean(t7ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_HEADER = 'Num_1000_n1em6_ba_Xy'\n",
    "\n",
    "file_name = MY_PATH + FILE_HEADER\n",
    "\n",
    "with open(file_name, 'rb') as myfile:\n",
    "    mylist = pickle.load(myfile)\n",
    "    \n",
    "m_ind = mylist[0]\n",
    "v_energy = mylist[1]\n",
    "ba_X = m_ind\n",
    "ba_y = v_energy\n",
    "\n",
    "# 因为是蒙卡的原因，需要扔掉一些点。这里扔掉每个seed产生的\n",
    "# 前 Num_drop = 10 个点\n",
    "# 生成的 tba_X 和 tba_y 是去掉这些点后重新排布得到的训练集\n",
    "\n",
    "Num0 = 1000\n",
    "Num = Num0 + 1\n",
    "Num_thread = 20\n",
    "Num_iteration = 200\n",
    "\n",
    "Num_drop = 10\n",
    "\n",
    "tba_X = np.zeros([(Num_iteration - Num_drop)*Num_thread, Num])\n",
    "tba_y = np.zeros([(Num_iteration - Num_drop)*Num_thread])\n",
    "\n",
    "for i1 in range(0, Num_iteration - Num_drop):\n",
    "    for i2 in range(0, Num_thread):\n",
    "        X = ba_X[i2*Num_iteration + i1 + Num_drop, :]\n",
    "        y = ba_y[i2*Num_iteration + i1 + Num_drop]\n",
    "        tba_X[i1*Num_thread + i2, :] = X\n",
    "        tba_y[i1*Num_thread + i2] = y\n",
    "\n",
    "t8ba_X = tba_X\n",
    "t8ba_y = tba_y\n",
    "tv = np.mean(t8ba_X, axis=0)\n",
    "v_base = np.arange(Num0 + 1) - Num0/2\n",
    "plt.figure()\n",
    "plt.plot(v_base, tv, '-r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "X_train = t0ba_X[0:1000, 0:1000]\n",
    "y_train = t0ba_y[0:1000]/1.0e8\n",
    "X_test = t0ba_X[3000:3800, 0:1000]\n",
    "y_test = t0ba_y[3000:3800]/1.0e8\n",
    "\n",
    "num_train = y_train.__len__()\n",
    "num_test = y_test.__len__()\n",
    "\n",
    "# mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 200\n",
    "#计算一共有多少个批次\n",
    "n_batch = num_train // batch_size\n",
    "num1 = 16 # 第一层多少个卷积核\n",
    "num2 = 4 # 第二层多少个卷积核\n",
    "num3 = 200 # 全连接层多少个神经元\n",
    "#num4 = 10\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1) #生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置值\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape '[batch, in_height, in_width, in_channels]'\n",
    "    #  tensor 是四维的，批次，长，宽，通道(黑白为1， 彩色为3)\n",
    "    #W filter /kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #  W 是滤波器，四维， 长，宽，输入，输出\n",
    "    # stride[0] = stride[3] = 1, strides[1]代表x方向的步长，stride[2]代表y方向的步长\n",
    "    #  步长\n",
    "    # padding: A 'string' from: '\"SAME\", \"VALID\"'\n",
    "    # SAME 会补0， VALID不补零\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize[1,x,y,1] 窗口大小。\n",
    "    return tf.nn.avg_pool(x,ksize=[1,1,2,1],strides=[1,1,2,1],padding='SAME')\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 1000]) # 28x28\n",
    "y = tf.placeholder(tf.float32, [None,]) \n",
    "\n",
    "# 改变x的格式转化为4D的向量[batch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x, [-1,1,1000,1]) # 复原原来的图片\n",
    "\n",
    "# 初始化第一个卷积层的权值和偏置值\n",
    "W_conv1 = weight_variable([1,4,1,num1]) # 5x5 的采样窗口， 32个卷积核从1个平面抽取特征\n",
    "                                      # 1 代表输入通道  32 代表输出32个特征平面\n",
    "b_conv1 = bias_variable([num1]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.tanh((conv2d(x_image, W_conv1) + b_conv1)/1.0) # 图片和权值传入，它自己就\n",
    "                                                         # 会算。\n",
    "check1 = tf.reduce_mean(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1) # 进行max_pooling  结果再pooling \n",
    "\n",
    "# 初始化第二个卷积层的权值和偏置值\n",
    "W_conv2 = weight_variable([1,4,num1,num2]) #5x5的采样窗口， 64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([num2]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把h_pool1的权值和向量进行卷积，再加上偏置值，然后用relu激活函数激活\n",
    "h_conv2 = tf.nn.tanh((conv2d(h_pool1,W_conv2) + b_conv2)/1.0)\n",
    "check2 = tf.reduce_mean(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) #进行max_pooling\n",
    "\n",
    "# 28x28 的图片第一次卷积之后还是28x28， 第一次池化之后变成了14x14\n",
    "# 第二次卷积之后变成14x14， 第二次池化之后变成了7x7\n",
    "# 经过上面操作后得到64张7x7的平面 \n",
    "\n",
    "# 1*784 的图片经过两次池化得到 1*784/4 = 1*196\n",
    "\n",
    "#初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([1*250*num2, num3]) #上一层有7*7*64个神经元， 全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([num3]) # 1024个节点\n",
    "\n",
    "# 把池化层2的输出层扁平化为1维\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,1*250*num2])\n",
    "# 求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.tanh((tf.matmul(h_pool2_flat,W_fc1) + b_fc1)/1.0) * 1.0\n",
    "# h_fc1 = tf.matmul(h_pool2_flat,W_fc1) + b_fc1\n",
    "check3 = tf.reduce_mean(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "# keep_prob 用来表示神经元的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "# 初始化第二个全连接层\n",
    "W_fc2 = weight_variable([num3,1]) # 1001个输出给1个输出\n",
    "b_fc2 = bias_variable([1])\n",
    "\n",
    "# 计算输出\n",
    "prediction = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss_max = tf.reduce_max(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4.0e-1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=20,decay_rate=0.5,\n",
    "                                           staircase=True)\n",
    "\n",
    "# train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "# # 计算输出\n",
    "# prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2) # 转化为概率输出\n",
    "# # 交叉熵代价函数\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "\n",
    "# 使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "# # 结果存放在一个布尔列表中\n",
    "# correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) # argmax 返回一维张量中最大值所在位置\n",
    "# # 求准确率\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(101):\n",
    "        for batch in range(n_batch):\n",
    "            # batch_xs,batch_ys = mnist.train.next_batch(batch_size) # 获取一个批次数据\n",
    "            batch_xs = X_train[batch*batch_size:(batch+1)*batch_size,]\n",
    "            batch_ys = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            # print([batch*batch_size,(batch+1)*batch_size])\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0}) # 70% 神经元工作\n",
    "            \n",
    "        acc = sess.run(loss,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        acc2 = sess.run(loss_max,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        print(\"Iter \" + str(epoch) + \" ,loss: \" + str(acc) + \", loss_max: \" + str(acc2) + \" lr:\" + str(pr_lr))\n",
    "        print(\"check1 =  \" + str(sess.run(check1,feed_dict={x:X_train, y:y_train, keep_prob:1.0})))\n",
    "        print(\"check2 =  \" + str(sess.run(check2,feed_dict={x:X_train, y:y_train, keep_prob:1.0})))\n",
    "        print(\"check3 =  \" + str(sess.run(check3,feed_dict={x:X_train, y:y_train, keep_prob:1.0})))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "X_train = t0ba_X[0:1000, 0:1000]\n",
    "y_train = t0ba_y[0:1000]/1.0e8\n",
    "X_test = t0ba_X[3000:3800, 0:1000]\n",
    "y_test = t0ba_y[3000:3800]/1.0e8\n",
    "\n",
    "y_train = (y_train - np.mean(y_train))/3.0/np.std(y_train)\n",
    "\n",
    "num_train = y_train.__len__()\n",
    "num_test = y_test.__len__()\n",
    "\n",
    "# mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "n_batch = num_train // batch_size\n",
    "num1 = 16 # 第一层多少个卷积核\n",
    "num2 = 16 # 第二层多少个卷积核\n",
    "num3 = 200 # 全连接层多少个神经元\n",
    "#num4 = 10\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1) #生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置值\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape '[batch, in_height, in_width, in_channels]'\n",
    "    #  tensor 是四维的，批次，长，宽，通道(黑白为1， 彩色为3)\n",
    "    #W filter /kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #  W 是滤波器，四维， 长，宽，输入，输出\n",
    "    # stride[0] = stride[3] = 1, strides[1]代表x方向的步长，stride[2]代表y方向的步长\n",
    "    #  步长\n",
    "    # padding: A 'string' from: '\"SAME\", \"VALID\"'\n",
    "    # SAME 会补0， VALID不补零\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize[1,x,y,1] 窗口大小。\n",
    "    return tf.nn.avg_pool(x,ksize=[1,1,2,1],strides=[1,1,2,1],padding='SAME')\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 1000]) # 28x28\n",
    "y = tf.placeholder(tf.float32, [None,]) \n",
    "\n",
    "# 改变x的格式转化为4D的向量[batch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x, [-1,1,1000,1]) # 复原原来的图片\n",
    "\n",
    "# 初始化第一个卷积层的权值和偏置值\n",
    "W_conv1 = weight_variable([1,4,1,num1]) # 5x5 的采样窗口， 32个卷积核从1个平面抽取特征\n",
    "                                      # 1 代表输入通道  32 代表输出32个特征平面\n",
    "b_conv1 = bias_variable([num1]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.elu((conv2d(x_image, W_conv1) + b_conv1)/1.0) # 图片和权值传入，它自己就\n",
    "                                                         # 会算。\n",
    "check1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "h_pool1 = max_pool_2x2(h_conv1) # 进行max_pooling  结果再pooling \n",
    "\n",
    "# 初始化第二个卷积层的权值和偏置值\n",
    "W_conv2 = weight_variable([1,4,num1,num2]) #5x5的采样窗口， 64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([num2]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把h_pool1的权值和向量进行卷积，再加上偏置值，然后用relu激活函数激活\n",
    "h_conv2 = tf.nn.elu((conv2d(h_pool1,W_conv2) + b_conv2)/1.0)\n",
    "\n",
    "check2 = conv2d(h_pool1,W_conv2) + b_conv2\n",
    "h_pool2 = max_pool_2x2(h_conv2) #进行max_pooling\n",
    "\n",
    "# 28x28 的图片第一次卷积之后还是28x28， 第一次池化之后变成了14x14\n",
    "# 第二次卷积之后变成14x14， 第二次池化之后变成了7x7\n",
    "# 经过上面操作后得到64张7x7的平面 \n",
    "\n",
    "# 1*784 的图片经过两次池化得到 1*784/4 = 1*196\n",
    "\n",
    "#初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([1*250*num2, num3]) #上一层有7*7*64个神经元， 全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([num3]) # 1024个节点\n",
    "\n",
    "# 把池化层2的输出层扁平化为1维\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,1*250*num2])\n",
    "\n",
    "\n",
    "# 求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.relu((tf.matmul(h_pool2_flat,W_fc1) + b_fc1)/1.0) * 1.0\n",
    "# h_fc1 = tf.matmul(h_pool2_flat,W_fc1) + b_fc1\n",
    "check3 = tf.matmul(h_pool2_flat,W_fc1) + b_fc1\n",
    "\n",
    "\n",
    "# keep_prob 用来表示神经元的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "# 初始化第二个全连接层\n",
    "W_fc2 = weight_variable([num3,1]) # 1001个输出给1个输出\n",
    "b_fc2 = bias_variable([1])\n",
    "\n",
    "# 计算输出\n",
    "prediction = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss_max = tf.reduce_max(tf.square(y - prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 4.0e-1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=10,decay_rate=0.7,\n",
    "                                           staircase=True)\n",
    "\n",
    "# train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "# # 计算输出\n",
    "# prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2) # 转化为概率输出\n",
    "# # 交叉熵代价函数\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "\n",
    "# 使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "# # 结果存放在一个布尔列表中\n",
    "# correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) # argmax 返回一维张量中最大值所在位置\n",
    "# # 求准确率\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    chk1 = sess.run(check1,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    chk2 = sess.run(check2,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    chk3 = sess.run(check3,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    mymap = sess.run(W_fc1)\n",
    "    plt.pcolor(mymap)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    mymap2 = sess.run(b_fc1)\n",
    "    plt.plot(mymap2, '*-')\n",
    "    plt.show()\n",
    "    mymap3 = chk3\n",
    "    plt.plot(mymap3[0,:], 'r*-')\n",
    "    plt.show()\n",
    "    mymap4 = sess.run(h_pool2_flat,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    plt.plot(mymap4[0,:], 'g.-')\n",
    "    plt.show()\n",
    "    print(\"check1: mean=\" + str(np.mean(chk1)) + ' std=' + str(np.std(chk1)) \n",
    "          + ' max=' + str(np.max(chk1)), ' min=' + str(np.min(chk1)))\n",
    "    print(\"check2: mean=\" + str(np.mean(chk2)) + ' std=' + str(np.std(chk2)) \n",
    "          + ' max=' + str(np.max(chk2)), ' min=' + str(np.min(chk2)))\n",
    "    print(\"check3: mean=\" + str(np.mean(chk3)) + ' std=' + str(np.std(chk3)) \n",
    "          + ' max=' + str(np.max(chk3)), ' min=' + str(np.min(chk3)))\n",
    "    for epoch in range(101):\n",
    "        for batch in range(n_batch):\n",
    "            # batch_xs,batch_ys = mnist.train.next_batch(batch_size) # 获取一个批次数据\n",
    "            batch_xs = X_train[batch*batch_size:(batch+1)*batch_size,]\n",
    "            batch_ys = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            # print([batch*batch_size,(batch+1)*batch_size])\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0}) # 70% 神经元工作\n",
    "            \n",
    "        acc = sess.run(loss,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        acc2 = sess.run(loss_max,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        print(\"Iter \" + str(epoch) + \" ,loss: \" + str(acc) + \", loss_max: \" + str(acc2) + \" lr:\" + str(pr_lr))\n",
    "        chk1 = sess.run(check1,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        chk2 = sess.run(check2,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        chk3 = sess.run(check3,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        mymap = sess.run(W_fc1)\n",
    "        plt.pcolor(mymap)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        mymap2 = sess.run(b_fc1)\n",
    "        plt.plot(mymap2, '*-')\n",
    "        plt.show()\n",
    "        mymap3 = chk3\n",
    "        plt.plot(mymap3[0,:], 'r*-')\n",
    "        plt.show()\n",
    "        mymap4 = sess.run(h_pool2_flat,feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        plt.plot(mymap4[0,:], 'g.-')\n",
    "        plt.show()\n",
    "        print(\"check1: mean=\" + str(np.mean(chk1)) + ' std=' + str(np.std(chk1)) \n",
    "              + ' max=' + str(np.max(chk1)), ' min=' + str(np.min(chk1)))\n",
    "        print(\"check2: mean=\" + str(np.mean(chk2)) + ' std=' + str(np.std(chk2)) \n",
    "              + ' max=' + str(np.max(chk2)), ' min=' + str(np.min(chk2)))\n",
    "        print(\"check3: mean=\" + str(np.mean(chk3)) + ' std=' + str(np.std(chk3)) \n",
    "              + ' max=' + str(np.max(chk3)), ' min=' + str(np.min(chk3)))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v1, v2 = tf.nn.moments(t0ba_y,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.min(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(t0ba_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(t0ba_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chk3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.pcolor(mymap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mymap4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrainning\n",
    "# 先行完整带输出版本\n",
    "\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = t0ba_X[0:1]\n",
    "y_train = X_train/2.0 + 0.25\n",
    "\n",
    "X_test = t0ba_X[1:2]\n",
    "y_test = X_test/2.0 + 0.25\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 1001\n",
    "# num_layer_3 = 1\n",
    "# num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, num_layer_1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1],stddev=0.1) + \n",
    "                       tf.eye(num_dimension)*0.5)\n",
    "# biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.25)\n",
    "biases_L1 = tf.Variable(tf.truncated_normal([1, num_layer_1], mean=0.25, stddev=0.1))\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.leaky_relu(Wx_plus_b_L1, alpha=0.2)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "\n",
    "prediction = L1_drop\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "# loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "# loss = -tf.reduce_sum(prediction*tf.log(y))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 2.0e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=100,decay_rate=1.0,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    V1 = sess.run(Weight_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    V1 = V1[0:-1:10, 0:-1:10]\n",
    "    b1 = sess.run(biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0}).reshape([num_layer_1,])\n",
    "    myinput = sess.run(tf.matmul(x, Weight_L1) + biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    myy =  sess.run(y, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    myprediction =  sess.run(prediction, feed_dict={x:X_train, y:y_train, keep_prob:1.0}) \n",
    "    myy2 =  sess.run(y, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "    myprediction2 =  sess.run(prediction, feed_dict={x:X_test, y:y_test, keep_prob:1.0}) \n",
    "    plt.pcolor(V1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.plot(b1,'*')\n",
    "    plt.show()\n",
    "    plt.plot(myinput[0,:], 'r*')\n",
    "    plt.show()\n",
    "    plt.plot(myy[0,:], 'b*')\n",
    "    plt.plot(myprediction[0,:],'y*')\n",
    "    plt.xlabel(\"Trainning\")\n",
    "    plt.show()\n",
    "    plt.plot(myy2[0,:], 'b*')\n",
    "    plt.plot(myprediction2[0,:],'y*')\n",
    "    plt.xlabel(\"Testing\")\n",
    "    plt.show()\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "            V1 = sess.run(Weight_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            V1 = V1[0:-1:10, 0:-1:10]\n",
    "            b1 = sess.run(biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0}).reshape([num_layer_1,])\n",
    "            myinput = sess.run(tf.matmul(x, Weight_L1) + biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            myy =  sess.run(y, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            myprediction =  sess.run(prediction, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            plt.pcolor(V1)\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            plt.plot(b1,'*')\n",
    "            plt.show()\n",
    "            plt.plot(myinput[0,:], 'r*')\n",
    "            plt.show()\n",
    "            plt.plot(myy[0,:], 'b*')\n",
    "            plt.plot(myprediction[0,:],'y*')\n",
    "            plt.xlabel(\"Trainning\")\n",
    "            plt.show()\n",
    "            myy2 =  sess.run(y, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "            myprediction2 =  sess.run(prediction, feed_dict={x:X_test, y:y_test, keep_prob:1.0}) \n",
    "            plt.plot(myy2[0,:], 'b*')\n",
    "            plt.plot(myprediction2[0,:],'y*')\n",
    "            plt.xlabel(\"Testing\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.eye(8)\n",
    "a = np.arange(8)\n",
    "a[0:-1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pretrainning\n",
    "# 不带输出的版本\n",
    "\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = t0ba_X[0:1000]\n",
    "y_train = X_train/2.0\n",
    "\n",
    "X_test = t0ba_X[3000:-1]\n",
    "y_test = X_test/2.0\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 1001\n",
    "# num_layer_3 = 1\n",
    "# num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, num_layer_1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1], stddev=1.0))\n",
    "# biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.1)\n",
    "biases_L1 = tf.Variable(tf.truncated_normal([1, num_layer_1], mean=0.2, stddev=0.1))\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.leaky_relu(Wx_plus_b_L1)\n",
    "# L1 = tf.nn.relu(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "\n",
    "prediction = L1_drop\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss = tf.reduce_mean(tf.abs(y - prediction))\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1.0e1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=10,decay_rate=0.5,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrainning\n",
    "# 先行完整带输出版本\n",
    "# 测试 dropout效果\n",
    "\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = t0ba_X[0:1]\n",
    "X_train = X_train - 0.5\n",
    "y_train = X_train\n",
    "\n",
    "X_test = t0ba_X[1:2]\n",
    "X_test = X_test - 0.5\n",
    "y_test = X_test\n",
    "\n",
    "# X_train = X_train**2\n",
    "# X_test = X_test**2\n",
    "# 每个批次的大小 每次放入100 个数据\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "num_train,num_dimension = X_train.shape\n",
    "n_batch = num_train // batch_size  # 整除\n",
    "\n",
    "\n",
    "# 定义神经网络的结构\n",
    "num_layer_1 = 1001\n",
    "# num_layer_3 = 1\n",
    "# num_layer_2 = 200\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, num_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, num_layer_1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)\n",
    "\n",
    "# 创建神经网路\n",
    "\n",
    "# 第一个中间层， num_dimension 入， num_layer_1 出\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([num_dimension, num_layer_1],mean=0.0, stddev=0.01) + \n",
    "                       tf.eye(num_dimension)*0.924)\n",
    "# biases_L1 = tf.Variable(tf.zeros([1,num_layer_1]) + 0.25)\n",
    "biases_L1 = tf.Variable(tf.truncated_normal([1, num_layer_1], mean=0.0, stddev=0.01))\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "\n",
    "prediction = L1_drop\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "loss_ms = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss = loss_ms + tf.contrib.layers.l2_regularizer(0.001)(Weight_L1) + tf.contrib.layers.l2_regularizer(4.0)(biases_L1)\n",
    "# loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "# loss = -tf.reduce_sum(prediction*tf.log(y))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1.0e-3\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=100,decay_rate=1.0,\n",
    "                                           staircase=True)\n",
    "\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    V1 = sess.run(Weight_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    V1 = V1[0:-1:10, 0:-1:10]\n",
    "    b1 = sess.run(biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0}).reshape([num_layer_1,])\n",
    "    myinput = sess.run(tf.matmul(x, Weight_L1) + biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    myy =  sess.run(y, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "    myprediction =  sess.run(prediction, feed_dict={x:X_train, y:y_train, keep_prob:1.0}) \n",
    "    myy2 =  sess.run(y, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "    myprediction2 =  sess.run(prediction, feed_dict={x:X_test, y:y_test, keep_prob:1.0}) \n",
    "    plt.pcolor(V1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.plot(b1,'*')\n",
    "    plt.show()\n",
    "    plt.plot(myinput[0,:], 'r*')\n",
    "    plt.show()\n",
    "    plt.plot(myy[0,:], 'b*')\n",
    "    plt.plot(myprediction[0,:],'y*')\n",
    "    plt.xlabel(\"Trainning\")\n",
    "    plt.show()\n",
    "    plt.plot(myy2[0,:], 'b*')\n",
    "    plt.plot(myprediction2[0,:],'y*')\n",
    "    plt.xlabel(\"Testing\")\n",
    "    plt.show()\n",
    "    for epoch in range(20001):\n",
    "        sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.5 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # sess.run(train_step, feed_dict={x:X_train, y:y_train, keep_prob:1.0, lr:0.05 * rnd.uniform(0.0, 1.0) * 1.0 ** epoch})\n",
    "        # prediction_value = sess.run(prediction, feed_dict={x:x_data})\n",
    "        test_loss = sess.run(loss_ms, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "        train_loss = sess.run(loss_ms, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "        pr_lr = sess.run(learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"lr00 \" + \"Iter \" +str(epoch) + \" , testing: \" + str(test_loss) + \"  training :\" + str(train_loss) + \" lr=\" + str(pr_lr))\n",
    "            V1 = sess.run(Weight_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            V1 = V1[0:-1:10, 0:-1:10]\n",
    "            b1 = sess.run(biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0}).reshape([num_layer_1,])\n",
    "            myinput = sess.run(tf.matmul(x, Weight_L1) + biases_L1, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            myy =  sess.run(y, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            myprediction =  sess.run(prediction, feed_dict={x:X_train, y:y_train, keep_prob:1.0})\n",
    "            plt.pcolor(V1)\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            plt.plot(b1,'*')\n",
    "            plt.show()\n",
    "            plt.plot(myinput[0,:], 'r*')\n",
    "            plt.show()\n",
    "            plt.plot(myy[0,:], 'b*')\n",
    "            plt.plot(myprediction[0,:],'y*')\n",
    "            plt.xlabel(\"Trainning\")\n",
    "            plt.show()\n",
    "            myy2 =  sess.run(y, feed_dict={x:X_test, y:y_test, keep_prob:1.0})\n",
    "            myprediction2 =  sess.run(prediction, feed_dict={x:X_test, y:y_test, keep_prob:1.0}) \n",
    "            plt.plot(myy2[0,:], 'b*')\n",
    "            plt.plot(myprediction2[0,:],'y*')\n",
    "            plt.xlabel(\"Testing\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
