{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter 0 ,Testing Accuracy0.3062\n",
      "Iter 1 ,Testing Accuracy0.6286\n",
      "Iter 2 ,Testing Accuracy0.6383\n",
      "Iter 3 ,Testing Accuracy0.6493\n",
      "Iter 4 ,Testing Accuracy0.7526\n",
      "Iter 5 ,Testing Accuracy0.7766\n",
      "Iter 6 ,Testing Accuracy0.7839\n",
      "Iter 7 ,Testing Accuracy0.7924\n",
      "Iter 8 ,Testing Accuracy0.8004\n",
      "Iter 9 ,Testing Accuracy0.8035\n",
      "Iter 10 ,Testing Accuracy0.8204\n",
      "Iter 11 ,Testing Accuracy0.8418\n",
      "Iter 12 ,Testing Accuracy0.853\n",
      "Iter 13 ,Testing Accuracy0.859\n",
      "Iter 14 ,Testing Accuracy0.8643\n",
      "Iter 15 ,Testing Accuracy0.8666\n",
      "Iter 16 ,Testing Accuracy0.8702\n",
      "Iter 19 ,Testing Accuracy0.8801\n",
      "Iter 20 ,Testing Accuracy0.8805\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "num1 = 4 # 第一层多少个卷积核\n",
    "num2 = 4 # 第二层多少个卷积核\n",
    "num3 = 10 # 全连接层多少个神经元\n",
    "#num4 = 10\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1) #生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置值\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape '[batch, in_height, in_width, in_channels]'\n",
    "    #  tensor 是四维的，批次，长，宽，通道(黑白为1， 彩色为3)\n",
    "    #W filter /kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #  W 是滤波器，四维， 长，宽，输入，输出\n",
    "    # stride[0] = stride[3] = 1, strides[1]代表x方向的步长，stride[2]代表y方向的步长\n",
    "    #  步长\n",
    "    # padding: A 'string' from: '\"SAME\", \"VALID\"'\n",
    "    # SAME 会补0， VALID不补零\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize[1,x,y,1] 窗口大小。\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28x28\n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "\n",
    "# 改变x的格式转化为4D的向量[batch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x, [-1,28,28,1]) # 复原原来的图片\n",
    "\n",
    "# 初始化第一个卷积层的权值和偏置值\n",
    "W_conv1 = weight_variable([5,5,1,num1]) # 5x5 的采样窗口， 32个卷积核从1个平面抽取特征\n",
    "                                      # 1 代表输入通道  32 代表输出32个特征平面\n",
    "b_conv1 = bias_variable([num1]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # 图片和权值传入，它自己就\n",
    "                                                         # 会算。\n",
    "h_pool1 = max_pool_2x2(h_conv1) # 进行max_pooling  结果再pooling \n",
    "\n",
    "# 初始化第二个卷积层的权值和偏置值\n",
    "W_conv2 = weight_variable([5,5,num1,num2]) #5x5的采样窗口， 64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([num2]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把h_pool1的权值和向量进行卷积，再加上偏置值，然后用relu激活函数激活\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) #进行max_pooling\n",
    "\n",
    "# 28x28 的图片第一次卷积之后还是28x28， 第一次池化之后变成了14x14\n",
    "# 第二次卷积之后变成14x14， 第二次池化之后变成了7x7\n",
    "# 经过上面操作后得到64张7x7的平面\n",
    "\n",
    "#初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([7*7*num2, num3]) #上一层有7*7*64个神经元， 全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([num3]) # 1024个节点\n",
    "\n",
    "# 把池化层2的输出层扁平化为1维\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,7*7*num2])\n",
    "# 求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)\n",
    "\n",
    "# keep_prob 用来表示神经元的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "# 初始化第二个全连接层\n",
    "W_fc2 = weight_variable([num3,10]) # 1024个输出给10个输出\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# 计算输出\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2) # 转化为概率输出\n",
    "\n",
    "# 交叉熵代价函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "# 使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 结果存放在一个布尔列表中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) # argmax 返回一维张量中最大值所在位置\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size) # 获取一个批次数据\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7}) # 70% 神经元工作\n",
    "            \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        print(\"Iter \" + str(epoch) + \" ,Testing Accuracy\" + str(acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter 0 ,Testing Accuracy0.1939\n",
      "Iter 1 ,Testing Accuracy0.62\n",
      "Iter 2 ,Testing Accuracy0.7469\n",
      "Iter 3 ,Testing Accuracy0.7699\n",
      "Iter 4 ,Testing Accuracy0.7811\n",
      "Iter 5 ,Testing Accuracy0.7925\n",
      "Iter 6 ,Testing Accuracy0.812\n",
      "Iter 7 ,Testing Accuracy0.82\n",
      "Iter 8 ,Testing Accuracy0.8351\n",
      "Iter 9 ,Testing Accuracy0.8348\n",
      "Iter 10 ,Testing Accuracy0.8394\n",
      "Iter 11 ,Testing Accuracy0.845\n",
      "Iter 12 ,Testing Accuracy0.848\n",
      "Iter 13 ,Testing Accuracy0.8506\n",
      "Iter 14 ,Testing Accuracy0.8505\n",
      "Iter 15 ,Testing Accuracy0.8531\n",
      "Iter 16 ,Testing Accuracy0.8578\n",
      "Iter 17 ,Testing Accuracy0.8597\n",
      "Iter 19 ,Testing Accuracy0.8625\n",
      "Iter 20 ,Testing Accuracy0.8636\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "num1 = 4 # 第一层多少个卷积核\n",
    "num2 = 4 # 第二层多少个卷积核\n",
    "num3 = 10 # 全连接层多少个神经元\n",
    "#num4 = 10\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1) #生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置值\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape '[batch, in_height, in_width, in_channels]'\n",
    "    #  tensor 是四维的，批次，长，宽，通道(黑白为1， 彩色为3)\n",
    "    #W filter /kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #  W 是滤波器，四维， 长，宽，输入，输出\n",
    "    # stride[0] = stride[3] = 1, strides[1]代表x方向的步长，stride[2]代表y方向的步长\n",
    "    #  步长\n",
    "    # padding: A 'string' from: '\"SAME\", \"VALID\"'\n",
    "    # SAME 会补0， VALID不补零\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize[1,x,y,1] 窗口大小。\n",
    "    return tf.nn.max_pool(x,ksize=[1,1,2,1],strides=[1,1,2,1],padding='SAME')\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28x28\n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "\n",
    "# 改变x的格式转化为4D的向量[batch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x, [-1,1,28*28,1]) # 复原原来的图片\n",
    "\n",
    "# 初始化第一个卷积层的权值和偏置值\n",
    "W_conv1 = weight_variable([1,5,1,num1]) # 5x5 的采样窗口， 32个卷积核从1个平面抽取特征\n",
    "                                      # 1 代表输入通道  32 代表输出32个特征平面\n",
    "b_conv1 = bias_variable([num1]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # 图片和权值传入，它自己就\n",
    "                                                         # 会算。\n",
    "h_pool1 = max_pool_2x2(h_conv1) # 进行max_pooling  结果再pooling \n",
    "\n",
    "# 初始化第二个卷积层的权值和偏置值\n",
    "W_conv2 = weight_variable([1,5,num1,num2]) #5x5的采样窗口， 64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([num2]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把h_pool1的权值和向量进行卷积，再加上偏置值，然后用relu激活函数激活\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) #进行max_pooling\n",
    "\n",
    "# 28x28 的图片第一次卷积之后还是28x28， 第一次池化之后变成了14x14\n",
    "# 第二次卷积之后变成14x14， 第二次池化之后变成了7x7\n",
    "# 经过上面操作后得到64张7x7的平面 \n",
    "\n",
    "# 1*784 的图片经过两次池化得到 1*784/4 = 1*196\n",
    "\n",
    "#初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([1*196*num2, num3]) #上一层有7*7*64个神经元， 全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([num3]) # 1024个节点\n",
    "\n",
    "# 把池化层2的输出层扁平化为1维\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,1*196*num2])\n",
    "# 求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)\n",
    "\n",
    "# keep_prob 用来表示神经元的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "# 初始化第二个全连接层\n",
    "W_fc2 = weight_variable([num3,10]) # 1024个输出给10个输出\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# 计算输出\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2) # 转化为概率输出\n",
    "\n",
    "# 交叉熵代价函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "# 使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 结果存放在一个布尔列表中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) # argmax 返回一维张量中最大值所在位置\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size) # 获取一个批次数据\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7}) # 70% 神经元工作\n",
    "            \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        print(\"Iter \" + str(epoch) + \" ,Testing Accuracy\" + str(acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter 0 ,Testing Accuracy0.2134\n",
      "Iter 1 ,Testing Accuracy0.3887\n",
      "Iter 2 ,Testing Accuracy0.4274\n",
      "Iter 3 ,Testing Accuracy0.578\n",
      "Iter 4 ,Testing Accuracy0.6596\n",
      "Iter 5 ,Testing Accuracy0.7012\n",
      "Iter 6 ,Testing Accuracy0.7254\n",
      "Iter 7 ,Testing Accuracy0.7482\n",
      "Iter 8 ,Testing Accuracy0.7629\n",
      "Iter 9 ,Testing Accuracy0.7944\n",
      "Iter 10 ,Testing Accuracy0.8207\n",
      "Iter 11 ,Testing Accuracy0.8291\n",
      "Iter 12 ,Testing Accuracy0.8384\n",
      "Iter 13 ,Testing Accuracy0.8439\n",
      "Iter 14 ,Testing Accuracy0.8472\n",
      "Iter 15 ,Testing Accuracy0.8546\n",
      "Iter 16 ,Testing Accuracy0.8545\n",
      "Iter 17 ,Testing Accuracy0.8598\n",
      "Iter 18 ,Testing Accuracy0.8619\n",
      "Iter 19 ,Testing Accuracy0.8636\n",
      "Iter 20 ,Testing Accuracy0.8662\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 100\n",
    "#计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "num1 = 2 # 第一层多少个卷积核\n",
    "num2 = 2 # 第二层多少个卷积核\n",
    "num3 = 10 # 全连接层多少个神经元\n",
    "#num4 = 10\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1) #生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置值\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape '[batch, in_height, in_width, in_channels]'\n",
    "    #  tensor 是四维的，批次，长，宽，通道(黑白为1， 彩色为3)\n",
    "    #W filter /kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #  W 是滤波器，四维， 长，宽，输入，输出\n",
    "    # stride[0] = stride[3] = 1, strides[1]代表x方向的步长，stride[2]代表y方向的步长\n",
    "    #  步长\n",
    "    # padding: A 'string' from: '\"SAME\", \"VALID\"'\n",
    "    # SAME 会补0， VALID不补零\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize[1,x,y,1] 窗口大小。\n",
    "    return tf.nn.max_pool(x,ksize=[1,1,2,1],strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28x28\n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "\n",
    "# 改变x的格式转化为4D的向量[batch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x, [-1,1,28*28,1]) # 复原原来的图片\n",
    "\n",
    "# 初始化第一个卷积层的权值和偏置值\n",
    "W_conv1 = weight_variable([1,5,1,num1]) # 5x5 的采样窗口， 32个卷积核从1个平面抽取特征\n",
    "                                      # 1 代表输入通道  32 代表输出32个特征平面\n",
    "b_conv1 = bias_variable([num1]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # 图片和权值传入，它自己就\n",
    "                                                         # 会算。\n",
    "h_pool1 = max_pool_2x2(h_conv1) # 进行max_pooling  结果再pooling \n",
    "\n",
    "# 初始化第二个卷积层的权值和偏置值\n",
    "W_conv2 = weight_variable([1,5,num1,num2]) #5x5的采样窗口， 64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([num2]) # 每一个卷积核一个偏置值\n",
    "\n",
    "# 把h_pool1的权值和向量进行卷积，再加上偏置值，然后用relu激活函数激活\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) #进行max_pooling\n",
    "\n",
    "# 28x28 的图片第一次卷积之后还是28x28， 第一次池化之后变成了14x14\n",
    "# 第二次卷积之后变成14x14， 第二次池化之后变成了7x7\n",
    "# 经过上面操作后得到64张7x7的平面 \n",
    "\n",
    "# 1*784 的图片经过两次池化得到 1*784/4 = 1*196\n",
    "\n",
    "#初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([1*784*num2, num3]) #上一层有7*7*64个神经元， 全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([num3]) # 1024个节点\n",
    "\n",
    "# 把池化层2的输出层扁平化为1维\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,1*784*num2])\n",
    "# 求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)\n",
    "\n",
    "# keep_prob 用来表示神经元的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "# 初始化第二个全连接层\n",
    "W_fc2 = weight_variable([num3,10]) # 1024个输出给10个输出\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# 计算输出\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2) # 转化为概率输出\n",
    "\n",
    "# 交叉熵代价函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "# 使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 结果存放在一个布尔列表中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) # argmax 返回一维张量中最大值所在位置\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size) # 获取一个批次数据\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7}) # 70% 神经元工作\n",
    "            \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        print(\"Iter \" + str(epoch) + \" ,Testing Accuracy\" + str(acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
